{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2s4kN_QPQVe"
   },
   "source": [
    "# **Recommender Systems**\n",
    "\n",
    "<img src=\"../images/recsys-posters.png\">\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Recommender_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Â© Deep Learning Indaba 2023. Apache License 2.0.\n",
    "\n",
    "**Authors:** [Amrit Purshotam](https://nl.linkedin.com/in/amritpurshotam)\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Recommender Systems are probably one of the most ubiquitous type of machine learning model that we encounter in our online life. They influence what we see in our social media feeds, the products we buy, the music we listen to, the food we eat, and the movies we watch. Sometimes they're so good that people feel that their phone is spying on their conversations! In this prac, we hope to convince you that this isn't the case (mostly) as well as taking you through some of the techniques popularly used in industry that recommends the content you see online by building our very own movie recommender system.\n",
    "\n",
    "**Topics:**\n",
    "\n",
    "Content: Machine Learning, Recommender Systems, Approximate Nearest Neighbours\n",
    "\n",
    "**Aims/Learning Objectives:**\n",
    "\n",
    "- General architecture of a recommender systems.\n",
    "- Techniques for making recommendations.\n",
    "- Serving recommendations efficiently in production.\n",
    "\n",
    "**Before you start:**\n",
    "\n",
    "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EqhIg1odqg0"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4boGA9rYdt9l"
   },
   "outputs": [],
   "source": [
    "## Install and import anything required. Capture hides the output from the cell.\n",
    "# @title Import required packages. (Run Cell)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from typing import Iterable, Mapping, Tuple, Sequence\n",
    "from textwrap import wrap\n",
    "\n",
    "import requests\n",
    "import jax\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from flax import struct\n",
    "from clu import metrics\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "import jax.tree_util as tree\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from flax.training import train_state\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TeWbblqpNutK"
   },
   "outputs": [],
   "source": [
    "# @title Helper methods\n",
    "\n",
    "def download_posters():\n",
    "    #url = \"https://drive.google.com/uc?id=15ipM74Qdc74d25jRQ3Mf4yB79tZqSkjU\"\n",
    "    url = \"https://drive.usercontent.google.com/download?id=15ipM74Qdc74d25jRQ3Mf4yB79tZqSkjU&authuser=0&confirm=t\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(\"../data/posters.zip\", mode=\"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=10*1024):\n",
    "            file.write(chunk)\n",
    "\n",
    "def unzip_posters():\n",
    "    with zipfile.ZipFile(\"../data/posters.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"../data/\")\n",
    "\n",
    "def get_dataset(ds_name: str) -> pd.DataFrame:\n",
    "  ds, info = tfds.load(f'movielens/{ds_name}', data_dir=\"../data\", with_info=True)\n",
    "  df = tfds.as_dataframe(ds['train'], info)\n",
    "  df = df.astype({'user_id': int, 'movie_id': int})\n",
    "  df.loc[:, 'movie_title'] = df['movie_title'].str.decode(\"utf-8\")\n",
    "  return df\n",
    "\n",
    "def cross_tabulate(df: pd.DataFrame, num_samples: int = 10) -> pd.DataFrame:\n",
    "  pivot_df = df.pivot(index='user_id', columns='movie_id', values='user_rating')\n",
    "  pivot_df = pivot_df.loc[df['user_id'].sample(num_samples), df['movie_id'].sample(num_samples)].dropna(axis=0, thresh=1).fillna(\"\")\n",
    "  return pivot_df\n",
    "\n",
    "def make_mapping(id_set: Iterable[str]) -> Mapping[str, int]:\n",
    "  return {id_str: i for (i, id_str) in enumerate(id_set)}\n",
    "\n",
    "def densify_column_values(df: pd.DataFrame, col_name: str) -> Tuple[pd.Series, Sequence[str]]:\n",
    "  col_values = sorted(set(df[col_name]))\n",
    "  col_ids_map = make_mapping(col_values)\n",
    "  return df[col_name].apply(lambda col_id: col_ids_map[col_id]), col_values, col_ids_map\n",
    "\n",
    "def to_tfds(df: pd.DataFrame) -> tf.data.Dataset:\n",
    "  fields = {\n",
    "    ('user_id', tf.int32),\n",
    "    ('item_id', tf.int32),\n",
    "    ('user_rating', tf.float32),\n",
    "    ('timestamp', tf.int32)\n",
    "  }\n",
    "\n",
    "  tensor_slices = {\n",
    "      field: tf.cast(df[field].values, dtype=field_type)\n",
    "      for field, field_type in fields\n",
    "  }\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "def get_train_val_split(df: pd.DataFrame):\n",
    "  val_df = df.sample(frac=0.2)\n",
    "  train_df = df[~df.index.isin(val_df.index)]\n",
    "\n",
    "  return train_df, val_df\n",
    "\n",
    "def prepare_dataloaders(train_df: pd.DataFrame, val_df: pd.DataFrame, batch_size: int, num_epochs: int):\n",
    "  train_ds = (\n",
    "    to_tfds(train_df)\n",
    "    .repeat(config.NUM_EPOCHS)\n",
    "    .shuffle(1024)\n",
    "    .batch(config.BATCH_SIZE, drop_remainder=False)\n",
    "    .prefetch(1)\n",
    "  )\n",
    "  val_ds = (\n",
    "    to_tfds(val_df)\n",
    "    .shuffle(1024)\n",
    "    .batch(config.BATCH_SIZE, drop_remainder=False)\n",
    "    .prefetch(1)\n",
    "  )\n",
    "  return train_ds, val_ds\n",
    "\n",
    "def plot_metric_history(metrics_history):\n",
    "  fig, ax1 = plt.subplots(1, 1, figsize=(7, 5))\n",
    "  ax1.set_title('Loss')\n",
    "  for dataset in ('train','val'):\n",
    "    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "  ax1.legend()\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "\n",
    "def make_movie_lookup(df: pd.DataFrame):\n",
    "    movies = df[['movie_id', 'movie_title']].drop_duplicates(subset='movie_id')\n",
    "    movie_lookup = dict(zip(movies.movie_id, movies.movie_title))\n",
    "    return movie_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bKVQDEz_eQWW"
   },
   "outputs": [],
   "source": [
    "# @title Config\n",
    "\n",
    "class Config:\n",
    "    DATASET = \"latest-small-ratings\" # all the options are 100k-ratings 1m-ratings 20m-ratings 25m-ratings latest-small-ratings\n",
    "    SEED = 42\n",
    "    EMB_DIM = 50\n",
    "    LR = 5e-3\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 5\n",
    "    USE_POSTERS = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "rng = jax.random.PRNGKey(config.SEED)\n",
    "rng, model_rng, dataset_rng = jax.random.split(rng, 3)\n",
    "\n",
    "df = get_dataset(config.DATASET)\n",
    "\n",
    "if config.USE_POSTERS:\n",
    "    download_posters()\n",
    "    unzip_posters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZUp8i37dFbU"
   },
   "source": [
    "## **1. Recommender Systems Overview**\n",
    "\n",
    "### **1.1 A Real World Scenario**\n",
    "\n",
    "Imagine you're going shopping for a new book. You enter the store and start walking around scanning the shelves of books. Something catches your interest, you pause, take the book down and inspect the cover, and perhaps also read the blurb. You continue to do this until you find something you like at which point you purchase the book and leave.\n",
    "\n",
    "Now imagine you've read the book and quite enjoyed it and you want to buy another one. You go back to the store, browse around, and occasionally inspect some books that catch your interest. An assistant this time approaches you asking if you need help. Gladly you accept, and you mention the books you were looking at as well as the one that you purchased recently. Since they've been working there a long time and have helped many customers, they now have a good idea of what you may like and recommends a short list of books for you to look at. You do so and eventually settle on one to purchase.\n",
    "\n",
    "**Exercise**. Let's pause here for a moment and dig deeper into this scenario.\n",
    "\n",
    "- From all the books in the store, what does it say about the ones you paused to look at, the ones you ignored, and the ones you purchased?\n",
    "- What does it say about you as the reader and your preferences? Could there be\n",
    "other people like you?\n",
    "- How was the assistant able to narrow down all the books in the store to just a few from which you actually purchased one?\n",
    "\n",
    "<img src=\"../images/book_store.png\" />\n",
    "\n",
    "Source: Kim Falk. *Practical Recommender Systems*. 2019. Manning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm3yxTa9pp8b"
   },
   "source": [
    "The answer to these questions is now getting into the heart of recommender systems. Your behaviour probably wasn't random but instead had some structure and logic to it. You also likely have particular preferences to some genres and/or authors. Without even knowing anything else about the books, this follows then that the books you looked at and purchased matched those preferences and the ones you ignored more likely did not. Additionally, based on your preferences, what was catching your interest, and what you previously read, the store assistant was able to stitch together a rough profile of you. She then thought about her previous customers similar to you and what they had previously purchased. This is how she was then able to shortlist relevant books for you to look at.\n",
    "\n",
    "As you probably guessed it by now, the store assistant is the recommender system in this example. Instead of a person though, we want to build something that is able to learn the latent structure between all the books, all our customers, and how well they match each other in order to then make our recommendations. In the following sections we will learn how to do this. But first let's go over the general architecture of a recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox8aHBxO4luZ"
   },
   "source": [
    "### **1.2 Recommendation System Architecture**\n",
    "\n",
    "#### **1.2.1 Overview**\n",
    "![picture](../images/recsys_overview.png)\n",
    "Source: [*Recommendation System Design*](https://medium.com/double-pointer/system-design-interview-recommendation-system-design-as-used-by-youtube-netflix-etc-c457aaec3ab). 2021. Medium.\n",
    "\n",
    "The above diagram generalises our previous scenario from books to searches, songs, and movies (note there will usually only be one) while the user is you as before. Notice how the user interacts with these *items* which then gets sent to the recommender system as feedback. The recommender system processes this, retrieves relevant items from it's database and then serves them to the user. The user, in turn, further interacts with these items and continues this loop until some terminating criteria. Perhaps they found a movie they like and started watching, or they purchase a book like in our previous example, or they simply leave.\n",
    "\n",
    "Finally, you may have noticed some new terminology in the diagram, the *query / context* on the user side of diagram. This is a further generalisation of what we're retrieving relevant items for. In this example it's the user but it can also include their previous searches, item interactions, the time of day, the device they're using, or any of combination of them.\n",
    "\n",
    "#### **1.2.2 Zooming in**\n",
    "\n",
    "Let's zoom in closer to the recommender system now by having a look at how YouTube described theirs back in 2016 in their seminal paper on the topic. Being YouTube, the items here would be videos which number in the millions (and most likely in the billions as of writing in 2023). Pay special attention to the blue stages, notice the number of items going into each stage going down and hence the funnel shape.\n",
    "\n",
    "Looking at the first one, the job of the *candidate generation* stage is to efficiently *retrieve* relevant items from your database (which is why you may find in the literature, this stage is also known as *retrieval*). Speed is of the utmost importance here so some leeway is allowed in terms of relevancy as long as the number of items we reduce down to is manageable for the downstream parts of the recommender. This lookup speed is achieved partly by the techniques we discuss and implement later in this practical but also by limiting the number of features that feed into this stage.\n",
    "\n",
    "The second *ranking* stage (also known as *scoring*) then takes these items and sorts them based on additional features that come from the user as well as the features of the item itself to optimise for some target we care about. In the case of YouTube, it will be for watch time, or in the case of an e-commerce website, the likelihood to purchase the items. Another reason for a ranking stage is that you may have multiple candidate generators and you now need a way to combine the results in some optimal way, at which point they're then shown to the user. It's also important to note, this ranking stage is not always necessary. Sometimes the retrieval step is good enough and you can keep the complexity of the system down, reduce implementation times, lower maintenance overhead, and therefore costs.\n",
    "\n",
    "![picture](../images/recsys_yt.png)\n",
    "\n",
    "Source: Covington et. al. [*Deep Neural Networks for YouTube Recommendations*](https://research.google/pubs/pub45530/). 2016. Proceedings of the 10th ACM Conference on Recommender Systems.\n",
    "\n",
    "Finally, this last diagram, courtesy of the recommendations team at NVIDIA, further expands on the above ideas by defining two more stages namely *filtering* and *ordering*. After the retrieval / candidate generation stage, we may find that some of the items, while relevant, aren't useful and so need to be filtered out. In an e-commerce scenario this could be an item that's out of stock or in the case of a social media platform, a post coming from a person or topic you've blocked / muted. The *ordering* step which takes place after ranking / scoring then refers to further refining the order of the items depending on some business logic. For example, promoting on sale items or perhaps even sponsored placements.\n",
    "\n",
    "![picture](../images/recsys_architecture.png)\n",
    "\n",
    "Source: Even Oldridge. [*Recommender Systems, Not Just Recommender Models*](https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e). 2022. Medium.\n",
    "\n",
    "The rest of this practical will now focus primarily on the retrieval / candidate generation stage of a recommender system. We hope this introduction and overview of recommender systems helps put into context the specific piece we will be building out. In particular, we will be learning about Collaborative Filtering and Graph Neural Networks for recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiXBlHjrb0DJ"
   },
   "source": [
    "## **2. Collaborative Filtering**\n",
    "\n",
    "### **2.1 Overview**\n",
    "\n",
    "Collaborative Filtering is a technique that learns how to recommend items to user A based on the interests of a similar user B by using similarities between these users and items simultaneously. This is opposed to content-based filtering that uses hand-engineered and/or explicit features of an item for further recommendations for e.g. recommending a movie in the same genre of one who you already watched.\n",
    "\n",
    "<img src=\"../images/cbcf.png\" width=\"100%\">\n",
    "\n",
    "Source: Arthur Mello. [*How do Netflix and Amazon know what I want?*](https://towardsdatascience.com/how-do-netflix-and-amazon-know-what-i-want-852c480b67ac). 2020. Medium.\n",
    "\n",
    "### **2.2 Matrix Factorisation**\n",
    "\n",
    "Consider a movie recommendation system in which the training data consists of a feedback matrix $A â R^{m Ã n}$ where $m$ is the number users and $n$ is the number of items. Each row then represents a user and each column represents an item (a movie) with the entries indicating a users rating of a particular movie. Our goal then is to learn\n",
    "- A user embedding matrix $U â \\mathbb{R}^{m Ã d}$, where row $i$ is the embedding for user $i$ and $d$ is the length of the embedding.\n",
    "- An item embedding matrix $V â \\mathbb{R}^{n Ã d}$, where row $j$ is the embedding for item $j$ and $d$ is the length of the embedding.\n",
    "\n",
    "such that their product $UV^{T}$ is a good approximation of the feedback matrix $A$. Note that the $(i, j)$ entry of $UV^{T}$ is simply the dot product of $â¨ U_i, V_j â©$ of the embeddings of user $i$ and item $j$ which you want to be as close as possible to $A_{ij}$. This process of finding these matrices $U$ and $V$ is known as *matrix factorisation*.\n",
    "\n",
    "<img src=\"../images/MatrixFactor.svg\" width=\"100%\" />\n",
    "\n",
    "\n",
    "Source: [*Google Recommendation Systems Course*](https://developers.google.com/machine-learning/recommendation/collaborative/matrix).\n",
    "\n",
    "Take special note how the matrices $U$ and $V$ give a more compact representation of the full matrix, even in this toy example (in practice $d$ is much smaller than $m$ and $n$ since you will have many more users and items). As a result, this process finds latent structure in the data without even requiring any knowledge of the movies themselves!\n",
    "\n",
    "Now, there are two common algorithms for finding these matrices, namely\n",
    "- Stochastic Gradient Descent (SGD) which is a generic method to minimise loss functions\n",
    "- Weighted Alternating Least Squares (WALS) which is specialised to this particular problem and works by alternating between fixing $U$ and solving for $V$ and vice versa.\n",
    "\n",
    "For this practical, we'll be using SGD to learn these matrices which means we have to define a loss function. One intuitive function is the squared distance. To do this, we minimise the average of squared errors over all pairs of observed entries.\n",
    "\n",
    "$ \\frac{1}{|obs|} â_{(i,j) \\in obs} (A_{ij} - â¨ U_i, V_j â©)^2$\n",
    "\n",
    "With this loss function defined, we can then randomly initialise our matrices $U$ and $V$, and through SGD iteratively update them until $UV^T$ is a good approximation of $A$. We now finally have all the pieces to move ahead with the rest of the tutorial.\n",
    "\n",
    "### **2.3 The MovieLens Dataset**\n",
    "\n",
    "Since we don't have access to an actual streaming service watch history, we will instead use a dataset called [MovieLens](https://grouplens.org/datasets/movielens/) and in particular the `latest-small-ratings` subset which contains 100 thousand ratings across 9 thousand movies and 600 users. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9WIKY4GeLM6"
   },
   "outputs": [],
   "source": [
    "df = get_dataset(config.DATASET)\n",
    "movies = df[['movie_id', 'movie_title']].drop_duplicates()\n",
    "movie_lookup = make_movie_lookup(df)\n",
    "df.head(5)[['user_id', 'movie_title', 'user_rating', 'timestamp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5DIRRBLhcK8"
   },
   "source": [
    "As you can see, we have our users represented by the `user_id`, the movies (our items), and the `user_rating` out of 5 that the user gave that particular movie. We also have the `timestamp` of when the user rated the movie in unix time format which is the number of seconds that elapsed since 1 January 1970 UTC and a common way of representing time due to it's unambiguity.\n",
    "\n",
    "Now what if we cross-tabulate a sample of this data to get an alternative view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NWEC-ZliR3T"
   },
   "outputs": [],
   "source": [
    "cross_tabulate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTTNRUf61drG"
   },
   "source": [
    "The table displayed shows some of the more popular movies and users. The empty cells are what we want our model to learn to fill in i.e. the movies we presume a user has not yet watched because they have yet to rate it. Then once we make these predictions, we can figure out which of those movies they're most likely to enjoy.\n",
    "\n",
    "Looking at this table, you may have also noticed it's sparsity. In reality, this table is actually even more so. Let's calculate how many cells are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fTuh9wV1ztg"
   },
   "outputs": [],
   "source": [
    "num_users = df['user_id'].unique().shape[0]\n",
    "num_items = df['movie_title'].unique().shape[0]\n",
    "num_ratings = df.shape[0]\n",
    "sparsity = 100 - (num_ratings / (num_users * num_items) * 100)\n",
    "print(f\"Sparsity: {sparsity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj9F3b3Cictf"
   },
   "source": [
    "\n",
    "\n",
    "### **2.2 Dataset Preparation**\n",
    "\n",
    "Now let's prepare our dataset for training. We will be mapping our users and movies from indexes starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2qg8FcoKt6r"
   },
   "outputs": [],
   "source": [
    "df['user_id'], user_list, user_to_id_mapping = densify_column_values(df, 'user_id')\n",
    "df['item_id'], movie_list, movie_to_id_mapping = densify_column_values(df, 'movie_title')\n",
    "df.head(5)[['user_id', 'item_id', 'user_rating', 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxuOf62RLQjf"
   },
   "source": [
    "Then we will split our data randomly into a train and validation set and create our dataloaders that will feed data into our training process later. The exact details aren't too important here but if you're interested feel free to inspect these methods in the Helper methods cell near the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekPnIBs0LPmY"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = get_train_val_split(df)\n",
    "train_ds, val_ds = prepare_dataloaders(train_df, val_df, config.BATCH_SIZE, config.NUM_EPOCHS)\n",
    "total_steps = train_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ACVEHMFU-kx"
   },
   "source": [
    "We are now ready to finally start implementing our Collaborative Filtering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9et6qbJeMRJ7"
   },
   "source": [
    "### **2.3 Building our Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LcnAgsROHew"
   },
   "source": [
    "#### **2.3.1 Dot Product Model**\n",
    "\n",
    "Let's start defining the architecture for this model in it's simplest formulation. Implement the below steps in the code cell below.\n",
    "1. Define $m$ and $n$ which correspond to the number of users and items respectively.\n",
    "2. Define $d$ corresponding to the dimension of the below embedding matrices.\n",
    "3. Create and specify the shape of our embedding matrices. Recall $U â \\mathbb{R}^{m Ã d}$ and $V â \\mathbb{R}^{n Ã d}$ for the user and item embeddings respectively. Hint: [`nn.Embed`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.Embed.html)\n",
    "4. Take the dot product of the user and item embeddings respectively. Hint: [`jnp.multiply`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.multiply.html) and [`jnp.sum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.sum.html) may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epY0Bx2wy38w"
   },
   "outputs": [],
   "source": [
    "class CFDotProduct(nn.Module):\n",
    "  # Step 1 and 2\n",
    "  # define the number of users (corresponds to m)\n",
    "  # define the number of items (corresponds to n)\n",
    "  # define the embedding dimension here (corresponds to d)\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # Step 3 Specify the shape of the embedding matrices\n",
    "    users = nn.Embed(num_embeddings=? features=?, name='user_embs')(x['user_id'])\n",
    "    items = nn.Embed(num_embeddings=? features=?, name='item_embs')(x['item_id'])\n",
    "\n",
    "    y = # Step 4 take the dot product of users and items\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iUoW5PZu2QrW"
   },
   "outputs": [],
   "source": [
    "# @title Solution\n",
    "class CFDotProduct(nn.Module):\n",
    "  num_users: int\n",
    "  num_items: int\n",
    "  emb_dim: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    users = nn.Embed(num_embeddings=self.num_users, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='user_embs')(x['user_id'])\n",
    "    items = nn.Embed(num_embeddings=self.num_items, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='item_embs')(x['item_id'])\n",
    "\n",
    "    y = jnp.sum(jnp.multiply(users, items), axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGp1mYjz548e"
   },
   "source": [
    "Now that we have specified the shape of the model, we now have to define our loss function. Recall that this will be the mean squared error between our predicted ratings and the actual ratings. Since this loss is a reasonable metric as well to measure the performance of our model, we'll use it as well. Hint: [`optax.squared_error`](https://optax.readthedocs.io/en/latest/api.html#optax.squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_07cxRfPybR"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn(params, batch)\n",
    "    loss = # fill in the loss function here and then take the mean from our batch\n",
    "    return loss\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "  logits = state.apply_fn(state.params, batch)\n",
    "  loss = # fill in the loss function here and then take the mean from our batch\n",
    "  metric_updates = state.metrics.single_from_model_output(logits=logits, labels=batch['user_rating'], loss=loss)\n",
    "  metrics = state.metrics.merge(metric_updates)\n",
    "  state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DZnuDwW76zYo"
   },
   "outputs": [],
   "source": [
    "# @title Solution\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn(params, batch)\n",
    "    loss = optax.squared_error(logits, batch['user_rating']).mean()\n",
    "    return loss\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "  logits = state.apply_fn(state.params, batch)\n",
    "  loss = optax.squared_error(logits, batch['user_rating']).mean()\n",
    "  metric_updates = state.metrics.single_from_model_output(logits=logits, labels=batch['user_rating'], loss=loss)\n",
    "  metrics = state.metrics.merge(metric_updates)\n",
    "  state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij-8RpAh9cnY"
   },
   "source": [
    "Now that we have our model and loss functions defined, let's initialise our training state and define our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20HhzWwwOuxO"
   },
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "def initialise_train_state(model, model_rng, dataloader, total_steps, weight_decay=None):\n",
    "  params = model.init(model_rng, next(tfds.as_numpy(dataloader).__iter__()))\n",
    "  scheduler = optax.cosine_onecycle_schedule(\n",
    "    transition_steps=total_steps,\n",
    "    peak_value=config.LR,\n",
    "    pct_start=0.25,\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=100000.0\n",
    "  )\n",
    "\n",
    "  if weight_decay:\n",
    "    tx = optax.adamw(learning_rate=scheduler, weight_decay=weight_decay)\n",
    "  else:\n",
    "    tx = optax.adam(learning_rate=scheduler)\n",
    "\n",
    "  state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=tx,\n",
    "    metrics=Metrics.empty()\n",
    "  )\n",
    "  return state\n",
    "\n",
    "def train_model(state, train_ds, val_ds):\n",
    "  num_steps_per_epoch = train_ds.cardinality().numpy() // config.NUM_EPOCHS\n",
    "\n",
    "  metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "  }\n",
    "\n",
    "  for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    state = train_step(state, batch)\n",
    "    state = compute_metrics(state=state, batch=batch)\n",
    "\n",
    "    if (step+1) % num_steps_per_epoch == 0:\n",
    "      for metric, value in state.metrics.compute().items():\n",
    "          metrics_history[f'train_{metric}'].append(value)\n",
    "      state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "      val_state = state\n",
    "      for val_batch in val_ds.as_numpy_iterator():\n",
    "          val_state = compute_metrics(state=val_state, batch=val_batch)\n",
    "\n",
    "      for metric, value in val_state.metrics.compute().items():\n",
    "          metrics_history[f'val_{metric}'].append(value)\n",
    "\n",
    "      print(\n",
    "          f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      )\n",
    "      print(\n",
    "          f\"val epoch: {(step+1) // num_steps_per_epoch}, \"\n",
    "          f\"loss: {metrics_history['val_loss'][-1]}, \"\n",
    "      )\n",
    "\n",
    "  return state, metrics_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhc8KLcl949i"
   },
   "source": [
    "We are now ready to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-amYerrPj7P"
   },
   "outputs": [],
   "source": [
    "model = CFDotProduct(num_users=len(user_list), num_items=len(movie_list), emb_dim=config.EMB_DIM)\n",
    "state = initialise_train_state(model, model_rng, train_ds, total_steps)\n",
    "state, metric_history = train_model(state, train_ds, val_ds)\n",
    "plot_metric_history(metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL3dBPBqOKQ8"
   },
   "source": [
    "Since our ratings are between `0.5` and `5`, the first thing we can do to make this model a bit better is to force the predictions to be in this range. A useful function for this would be the [`sigmoid`](https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.sigmoid.html) which constrains it's input to between `0` and `1`. Multiplying and shifting this then allows us to move these values to any range of values. Let's choose `0` and `5.5` since empirically these work better by expanding the range of allowed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7stlwPFvUqsO"
   },
   "outputs": [],
   "source": [
    "class CFWithRange(nn.Module):\n",
    "  num_users: int\n",
    "  num_items: int\n",
    "  emb_dim: int\n",
    "  # define the min_rating\n",
    "  # define the max_rating\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    user_embeds = nn.Embed(num_embeddings=self.num_users, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='user_embs')(x['user_id'])\n",
    "    item_embeds = nn.Embed(num_embeddings=self.num_items, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='item_embs')(x['item_id'])\n",
    "\n",
    "    y = jnp.sum(jnp.multiply(user_embeds, item_embeds), axis=1)\n",
    "    y = # constrain y to between 0 and 1\n",
    "    y = # expand the range to between 0 and 5.5.\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KtCw1RXrQ6-z"
   },
   "outputs": [],
   "source": [
    "# @title Solution\n",
    "class CFWithRange(nn.Module):\n",
    "  num_users: int\n",
    "  num_items: int\n",
    "  emb_dim: int\n",
    "  min_rating: float\n",
    "  max_rating: float\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    user_embeds = nn.Embed(num_embeddings=self.num_users, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='user_embs')(x['user_id'])\n",
    "    item_embeds = nn.Embed(num_embeddings=self.num_items, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='item_embs')(x['item_id'])\n",
    "\n",
    "    y = jnp.sum(jnp.multiply(user_embeds, item_embeds), axis=1)\n",
    "    y = nn.sigmoid(y)\n",
    "    y = y * (self.max_rating - self.min_rating) + self.min_rating\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdFbH29kSUO3"
   },
   "source": [
    "We're now ready to train the model again. Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o-dswpxVB41"
   },
   "outputs": [],
   "source": [
    "model = CFWithRange(\n",
    "  num_users=len(user_list),\n",
    "  num_items=len(movie_list),\n",
    "  emb_dim=config.EMB_DIM,\n",
    "  min_rating=0,\n",
    "  max_rating=5.5\n",
    ")\n",
    "state = initialise_train_state(model, model_rng, train_ds, total_steps)\n",
    "state, metric_history = train_model(state, train_ds, val_ds)\n",
    "plot_metric_history(metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgK5pcdzUblJ"
   },
   "source": [
    "If everything ran correctly, this should have performed a bit better. A missing piece now is that some users are just more positive or negative in their ratings than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi or very action-oriented, then you don't really have any way to say whether most people like it.\n",
    "\n",
    "That's because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and the same for each movie, it will handle this missing piece. Let's adjust our model architecture again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ybIOHsUMZao"
   },
   "source": [
    "#### **2.3.2 Dot Product with Bias Model**\n",
    "\n",
    "For each user and for each item, let's add a single number. This corresponds to one dimensional embeddings each the length of the number of users and number of items respectively. Let's add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kktEX-2qUkTa"
   },
   "outputs": [],
   "source": [
    "class CFDotProductBias(nn.Module):\n",
    "  num_items: int\n",
    "  num_users: int\n",
    "  emb_dim: int\n",
    "  min_rating: float\n",
    "  max_rating: float\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    user_embeds = nn.Embed(num_embeddings=self.num_users, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='user_embs')(x['user_id'])\n",
    "    item_embeds = nn.Embed(num_embeddings=self.num_items, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='item_embs')(x['item_id'])\n",
    "    # Add the 1 dimensional bias terms here\n",
    "    user_bias = nn.Embed(num_embeddings=?, features=?, embedding_init=nn.initializers.normal(), name='user_bias')(x['user_id'])\n",
    "    item_bias = nn.Embed(num_embeddings=?, features=?, embedding_init=nn.initializers.normal(), name='item_bias')(x['item_id'])\n",
    "\n",
    "    y = jnp.sum(jnp.multiply(user_embeds, item_embeds), axis=1, keepdims=1)\n",
    "    y += # add the biases\n",
    "    y = nn.sigmoid(y) * (self.max_rating - self.min_rating) + self.min_rating\n",
    "    return jnp.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MaEhkopMVxnI"
   },
   "outputs": [],
   "source": [
    "# @title Solution\n",
    "class CFDotProductBias(nn.Module):\n",
    "  num_items: int\n",
    "  num_users: int\n",
    "  emb_dim: int\n",
    "  min_rating: float\n",
    "  max_rating: float\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    item_embeds = nn.Embed(num_embeddings=self.num_items, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='item_embs')(x['item_id'])\n",
    "    item_bias = nn.Embed(num_embeddings=self.num_items, features=1, embedding_init=nn.initializers.normal(), name='item_bias')(x['item_id'])\n",
    "    user_embeds = nn.Embed(num_embeddings=self.num_users, features=self.emb_dim, embedding_init=nn.initializers.normal(), name='user_embs')(x['user_id'])\n",
    "    user_bias = nn.Embed(num_embeddings=self.num_users, features=1, embedding_init=nn.initializers.normal(), name='user_bias')(x['user_id'])\n",
    "\n",
    "    y = jnp.sum(jnp.multiply(user_embeds, item_embeds), axis=1, keepdims=1)\n",
    "    y += user_bias + item_bias\n",
    "    y = nn.sigmoid(y) * (self.max_rating - self.min_rating) + self.min_rating\n",
    "    return jnp.squeeze(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D0TmYgFXqlh"
   },
   "source": [
    "Let's train our model and see how this one performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTy0-BNBVL-I"
   },
   "outputs": [],
   "source": [
    "model = CFDotProductBias(\n",
    "  num_items=len(movie_list),\n",
    "  num_users=len(user_list),\n",
    "  emb_dim=config.EMB_DIM,\n",
    "  min_rating=0.0,\n",
    "  max_rating=5.5\n",
    ")\n",
    "state = initialise_train_state(model, model_rng, train_ds, total_steps)\n",
    "state, metric_history = train_model(state, train_ds, val_ds)\n",
    "plot_metric_history(metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMryLaMquEXA"
   },
   "source": [
    "##### **2.3.2.1 Mitigate overfitting**\n",
    "\n",
    "It appears our model has started to overfit since our validation loss started getting worse while our training loss continued to decrease. One way to mitigate this is to constrain our model with weight decay. The details aren't important right now but the main idea is that this technique prevents our weights from getting too high, a symptom of overfitting. Luckily this time, we don't even need to change our model, we can specify this as a parameter in our state initialisation step and the details are taken care of for you. Let's try training our model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixWY5i8gVeM0"
   },
   "outputs": [],
   "source": [
    "model = CFDotProductBias(\n",
    "  num_items=len(movie_list),\n",
    "  num_users=len(user_list),\n",
    "  emb_dim=config.EMB_DIM,\n",
    "  min_rating=0.0,\n",
    "  max_rating=5.5\n",
    ")\n",
    "state = initialise_train_state(model, model_rng, train_ds, total_steps, weight_decay=0.1)\n",
    "state, metric_history = train_model(state, train_ds, val_ds)\n",
    "plot_metric_history(metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwIpN1YMZFHT"
   },
   "source": [
    "Great, hopefully this should have led to an improved model and stabilised our validation loss. Now let's try to interpret the model we just built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34kiVmBcNKxi"
   },
   "source": [
    "### **2.4 Interpreting our Model**\n",
    "\n",
    "#### **2.4.1 Biases**\n",
    "\n",
    "We mentioned earlier that the biases we added to the model would be able to encode whether some users are generally more positive or negative in their reviews and whether some movies are universally better or worse than others. Let's put this to the test by inspecting the biases. First we'll look at the worst movies i.e. the movies with the lowest biases\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_movie_df = df.drop_duplicates(subset='movie_id')[['item_id', 'movie_id']]\n",
    "item_id_to_movie_id_mapping = dict(zip(item_movie_df.item_id, item_movie_df.movie_id))\n",
    "\n",
    "id_to_movie_mapping = {}\n",
    "for movie, id in movie_to_id_mapping.items():\n",
    "    id_to_movie_mapping[id] = movie\n",
    "\n",
    "def display_recommendations(item_ids: list[int]):\n",
    "    if config.USE_POSTERS:\n",
    "        movie_ids = [item_id_to_movie_id_mapping[item_id] for item_id in item_ids]\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(20, 7), dpi=80, sharex=True, sharey=True)\n",
    "        for i, movie_id in enumerate(movie_ids):\n",
    "            poster_path = f'../data/posters/{movie_id}.jpg'\n",
    "            if not os.path.isfile(poster_path):\n",
    "                poster_path = f'../data/posters/default.jpg'\n",
    "            image = plt.imread(poster_path)\n",
    "            ax[i].imshow(image)\n",
    "            ax[i].axis('off')\n",
    "            ax[i].set_title(\"\\n\".join(wrap(movie_lookup[movie_id], 35)))\n",
    "    else:\n",
    "        for item_id in item_ids:\n",
    "            print(id_to_movie_mapping[item_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggU13zqUb2ej"
   },
   "outputs": [],
   "source": [
    "item_ids = state.params['params']['item_bias']['embedding'].squeeze().argsort().tolist()[:5]\n",
    "display_recommendations(item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HniO_q9ocDDb"
   },
   "source": [
    "Hopefully none of these are any of your favourites!\n",
    "\n",
    "Let's now look at what the model learned to be some of the best movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl1FaeLUccOX"
   },
   "outputs": [],
   "source": [
    "item_ids = state.params['params']['item_bias']['embedding'].squeeze().argsort().tolist()[-5:]\n",
    "display_recommendations(item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNsww4ZVckCF"
   },
   "source": [
    "Do you recognise any of these and if so, do you agree with the model that these movies are generally universally loved? If you haven't watched any of them, perhaps you can choose one of these the next time you're looking for something and put this model to the test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoqRWHL4bzcY"
   },
   "source": [
    "#### **2.4.2 Embedding Distance**\n",
    "\n",
    "Our embeddings aren't quite so easy to directly interpret since there's too many factors to look at. We can however measure similarity between them. More formally a similarity measure is a function $s : E Ã E â \\mathbb{R}$ that takes a pair of embeddings and returns a scalar measuring their similarity. The embeddings can be used for candidate generation as follows: given a query embedding $q \\in E$, the system looks for item embeddings $x \\in E$ that are close to q i.e. embedding with high similarity $s(q,x)$. To determine similarity, most recommendation systems rely on one or more of the following:\n",
    " - **Cosine**: the cosine of the angle between the two vectors $s(q,x) = cos(q,x)$\n",
    " - **Dot Product**: the dot product of the two vectors $s(q,x) = â_{i=1}^{d} q_i x_i$\n",
    " - **Euclidean distance**: The distance in Euclidean space, $s(q,x) = ||q - x|| = [â_{i=1}^{d} (q_i - x_i)^2]^{\\frac{1}{2}}$\n",
    "\n",
    " For the purposes of this practical, let's choose cosine similarity for our recommender. We first define the function that will calculate the cosine similarity between our selected item embedding and all the other embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvwqKjU8k8Nz"
   },
   "outputs": [],
   "source": [
    "def get_cos_sim_distances(item, embeddings):\n",
    "  dot_products = jnp.sum(item[None] * embeddings, axis=1)\n",
    "  norm_item = jnp.linalg.norm(item)\n",
    "  norm_all = jnp.linalg.norm(embeddings, axis=1)\n",
    "  distances = (dot_products / (norm_item * norm_all))\n",
    "  return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKVdtYVA9PUV"
   },
   "source": [
    "Let's define function that will find the most similar movies for one we specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw_KmFZaliIl"
   },
   "outputs": [],
   "source": [
    "def get_top_k_most_similar_movies(movie_name, embeddings, k):\n",
    "  movie_id = movie_to_id_mapping[movie_name] # retrieve the movie_id\n",
    "  distances = get_cos_sim_distances(embeddings[movie_id], embeddings) # calculate the distances between the movie and all others\n",
    "  # Sort ascending and take the last k movie ID's (excluding the movie itself which will be the most similar i.e. angle = 0)\n",
    "  # Then reverse the order so the most similar movie IDs are first\n",
    "  return list(reversed(distances.argsort()[-k-1:-1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYX1nsYflnMd"
   },
   "source": [
    "Let's call this method now and try it out on the second Avengers movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_aYfnCCmV_T"
   },
   "outputs": [],
   "source": [
    "similar_ids = get_top_k_most_similar_movies(\"Amazing Spider-Man, The (2012)\", state.params['params']['item_embs']['embedding'], 5)\n",
    "display_recommendations(similar_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luFRDTuzncHW"
   },
   "source": [
    "Try testing out with your own movies. To find the exact name of a particular movie, here's a little helper function that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to search and display results\n",
    "def search_dataframe(search_text):\n",
    "    # Filter DataFrame based on search_text in the 'Name' column\n",
    "    filtered_df = movies[movies['movie_title'].str.contains(search_text, case=False, na=False)]\n",
    "    # Display the filtered DataFrame\n",
    "    display(filtered_df)\n",
    "\n",
    "# Create a text input widget\n",
    "search_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a movie name...',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create an output widget to display the results\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define an event handler function for the search input widget\n",
    "def on_search_change(change):\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "        search_text = change['new']\n",
    "        search_dataframe(search_text)  # Search and display results\n",
    "\n",
    "# Attach the event handler to the text input widget\n",
    "search_input.observe(on_search_change, names='value')\n",
    "\n",
    "# Display the input widget and the output\n",
    "display(search_input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmBWE07ohRPW"
   },
   "source": [
    "We can also visualise our embeddings using TensorBoard's [Embedding Projector](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin) where we can see where in this space all our movie embeddings lie. Let's load the tool. Towards the bottom left you should see some tabs called UMAP, t-SNE, and PCA. The details of these techniques are out of scope but their basic idea is to cluster our embeddings into a lower dimensional space that we can visualise.\n",
    "\n",
    "Try clicking around in the point clouds to see the similar movies around the point you select. You can also search for (and then click) for movies on the right. You can also reduce (or increase) the number of movies that are highlighted by tweaking tue `neighbours` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FNqDkxnzdWRr"
   },
   "outputs": [],
   "source": [
    "# @title Launch Tensorboard Projector\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"./tensorboard/\")\n",
    "writer.add_embedding(np.asarray(state.params['params']['item_embs']['embedding']), metadata=list(id_to_movie_mapping.values()))\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_ZxySJNNLpj"
   },
   "source": [
    "### **2.5 Making Recommendations**\n",
    "\n",
    "Apart from finding similar movies based purely on their similarity to each other, we can also find the movies in the neighbourhood of users i.e. the movies most similar to a user. This may seem surprising since users and movies are different entities. But a way to think about this is that our embedding space is an abstract representation common to both in which we can measure similarity using a similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kofc-7Ne9z97"
   },
   "outputs": [],
   "source": [
    "user_id = df['user_id'].sample(1).iloc[0] # select a random user\n",
    "item_embs = state.params['params']['item_embs']['embedding'] # retrieve our trained movie embeddings\n",
    "user_emb = state.params['params']['user_embs']['embedding'][user_id] # retrieve our trained user embedding\n",
    "distances = get_cos_sim_distances(user_emb, item_embs) # calculate the distance between our user and all the movies\n",
    "already_watched_movie_ids = df[df['user_id'] == user_id]['item_id'].tolist() # retrieve all the movies the user already watched\n",
    "rec_count = 0\n",
    "# Loop over the closest movies to the user that they have not yet watched\n",
    "for id in reversed(distances.argsort().tolist()):\n",
    "    if id not in already_watched_movie_ids and rec_count < 10:\n",
    "        print(id_to_movie_mapping[id])\n",
    "        rec_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2XmuYv2-dQL"
   },
   "source": [
    "Since our model is trained to predict ratings of movies, let's actually use it directly to predict the ratings a particular user would give to all the movies and recommend the movies they would have rated the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm0XNZaH-kWI"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def pred_step(state, batch):\n",
    "  logits = state.apply_fn(state.params, batch)\n",
    "  return logits\n",
    "\n",
    "def create_eval_tfds_from_df(df: pd.DataFrame) -> tf.data.Dataset:\n",
    "  eval_fields = {\n",
    "      ('item_id', tf.int32),\n",
    "      ('user_id', tf.int32),\n",
    "  }\n",
    "  tensor_slices = {\n",
    "      field: tf.cast(df[field].values, dtype=field_type)\n",
    "      for field, field_type in eval_fields\n",
    "  }\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "# prepare our data by mapping the same user id to all our movies\n",
    "all_items = df['item_id'].unique()\n",
    "eval_df = pd.DataFrame({'user_id': [user_id] * all_items.shape[0], 'item_id': all_items})\n",
    "eval_ds = create_eval_tfds_from_df(eval_df).batch(eval_df.shape[0], drop_remainder=False)\n",
    "\n",
    "# make the prediction on this data which returns the ratings\n",
    "preds = pred_step(state, eval_ds.as_numpy_iterator().next())\n",
    "eval_df['preds'] = pd.DataFrame(preds)\n",
    "\n",
    "# Display the top 10 rated movies that the user has not yet watched\n",
    "count = 0\n",
    "for row in eval_df.sort_values('preds', ascending=False).itertuples():\n",
    "  if row.item_id not in already_watched_movie_ids and count < 10:\n",
    "    print(f\"{row.preds:.2f}\", id_to_movie_mapping[row.item_id])\n",
    "    count += 1\n",
    "  elif count == 10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W5tQW6oD_9Z"
   },
   "source": [
    "### **2.6. Approximate Nearest Neighbours**\n",
    "\n",
    "So far in this practical we've been performing an *exhaustive search* for the most similar movies i.e. we've been calcuating the distance between a movie and every other movie. What if we have millions of movies? And millions of users logging into our streaming service for which we need to make these calculations? This will indeed get very costly in both time and resources. We need a way to perform these calculations much more efficiently. This is where *Approximate Nearest Neighbours* comes in. The idea is we tradeoff a small degree of accuracy in our similarity search but in exchange gain an enormous amount of performance (orders of magnitude).\n",
    "\n",
    "One such implementation of this idea, originally developed at Spotify, is called [*Approximate Nearest Neighbours Oh Yeah*](https://github.com/spotify/annoy) (yes, really) or more commonly known by it's acronym, *ANNOY*. The algorithm works by recursively splitting our embedding space by random hyperplanes, where each hyperplane is represented by a node in a tree, until there are at most $k$ items at every leaf node. In order to find the nearest neighbours for a specific embedding, the tree is traversed by calculating on which side of the hyperplane the point lies at every node and then returning the items at the final node. See the below image to get an idea of what the splitting looks like in a 2D space and the resulting tree.\n",
    "\n",
    "<img src=\"../images/tree-full-K-1024x793.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"../images/tree-full-K-graphviz1-1024x404.png\" width=\"60%\">\n",
    "\n",
    "Source: Erik Bernhardsson. [*Nearest neighbors and vector models*](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html). 2015.\n",
    "\n",
    "There are a few more details to this algorithm which are out of scope for this prac but if you'd like to learn more about it's inner workings, read this excellent [blog post](https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html), by the author himself of ANNOY, Erik Bernhardsson.\n",
    "\n",
    "Let's now create our ANNOY index. This process is slow and can take up to 10 minutes to run (perhaps you can read the blog post while you wait).\n",
    "\n",
    "We first create the `AnnoyIndex` where we specify the size of our embedding (the $d$ from earlier) and the distance metric. Here `angular` refers to the cosine similarity. We then loop over our embeddings, adding them to our index.\n",
    "\n",
    "Next we build our index by specifying the number of trees it should make. In the earlier explanation, we showed one tree being built but in reality the algorithm actually makes use of many. More trees results in greater accuracy but require more memory. In practice you will specify the number of trees that either max out your memory or until your desired level of accuracy is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mchj5wuAHYD"
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "t = AnnoyIndex(config.EMB_DIM, 'angular')\n",
    "for i, emb in tqdm(enumerate(state.params['params']['item_embs']['embedding']), total=state.params['params']['item_embs']['embedding'].shape[0]):\n",
    "    t.add_item(i, emb)\n",
    "\n",
    "t.build(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i76qkaPCF7IN"
   },
   "source": [
    "Let's again look at the most similar movies to this Avengers movie. We do this by calling the `get_nns_by_item` method and specifying the movie we would like to look up, the `n` (approximate) nearest neighbours we'd like to return, and `search_k` which refers to the number of embeddings it will consider for the distance calculation. The tradeoff made is that the higher the `search_k`, the more accurate it will be but the slower the performance. So in practice this number will be chosen by making sure it's as high as possible to maximise accuracy while making sure the results are returned in a time under the threshold you set for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR9wTX5ZAYkr"
   },
   "outputs": [],
   "source": [
    "movie_id = movie_to_id_mapping[\"Avengers: Age of Ultron (2015)\"]\n",
    "for id in t.get_nns_by_item(movie_id, 5, search_k=10):\n",
    "  print(id_to_movie_mapping[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWrM-ucnHM3u"
   },
   "source": [
    "Let's compare the results to the exact search we performed before. How does it compare? What if we tweak the `search_k` above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuAWyJzxDEyu"
   },
   "outputs": [],
   "source": [
    "similar_ids = get_top_k_most_similar_movies(\"Avengers: Age of Ultron (2015)\", state.params['params']['item_embs']['embedding'], 10)\n",
    "for id in similar_ids:\n",
    "  print(id_to_movie_mapping[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfX_PVxWSEOL"
   },
   "source": [
    "### **2.7. Types of Feedback**\n",
    "\n",
    "To build recommender systems, there are two types of feedback we train our models against.\n",
    "- **Explicit**: users specify whether or not they liked an item and sometimes even by how much. Examples of this are a thumbs up or thumbs down on a youtube video, review scores of a product, or even a movie rating!\n",
    "- **Implicit**: the feedback is inferred based on how the users are interacting with the items. For example clicking an item, watching a video video, adding a product to your basket and/or purchasing.\n",
    "\n",
    "The key differences between them is that explicit feedback is more sparse but gives you a higher confidence in to the users intent while implicit data is abundant but also noisier.\n",
    "\n",
    "Usually both types of feedback are collected in the online services you use. Explicit feedback comes from the users naturally using the product (for e.g. reviewing the product) while the implicit data is usually coming from the telemetry infrastructure monitoring how users are interacting with your application. Depending on the maturity of this infrastructure, this can simply be logging what you're clicking on but can also include timing how long you've been on a page, the location your web requests are originating from, tracking exactly what content you've been exposed to, and even whether you simply hovered over an item and for how long. All of this data is logged and dumped into massive data stores to be later cleaned, analysed, and eventually trained on to create the recommender models.\n",
    "\n",
    "Now we can actually answer the question posed in the title of this prac. These massive datasets comprised of both implicit and explicit feedback along with the large recommender models trained on them is how the recommendations can sometimes feels so good. So while your phone is not actually listening in on your conversation where you might have expressed some interest in those items, you more than likely also showed that interest implicitly by the simple usage of these internet platforms and this is all that was needed to figure out your preferences (especially when keeping in mind the thousands or even millions of people who might have interacted with the platform in similar ways). Now keep in mind how large these datasets and models can become. In this prac we built relatively small models on just 1MB of data and were still able to make plausible recommendations. They only get better when you scale them up.\n",
    "\n",
    "Hopefully now that your mind is at ease that you're not being spied on, we hope that you're now not nervous about how much of your usage data is being logged. At least no you can make a more informed decision about whether or not the quality of the recommendations you get is worth this tradeoff. Keep in mind, the purpose of these systems is to help you find the content you're looking for and discover content you might not have otherwise found. If this is still not worth it for you, luckily regulations are catching up and that it may start to become mandatory for to allow users to opt out of personalised recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV3YG7QOZD-B"
   },
   "source": [
    "## Conclusion\n",
    "**Summary:**\n",
    "\n",
    "In this prac we introduced the general architecture of recommender systems and specifically focused on retrieving relevant items for users. We did this by demonstrating how Colaborative Filtering could be used for this task. Lastly we also described the type of data that's collected and how it's all crunched to construct the datasets used to build the recommender models.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    " - [Eugene Yan](https://eugeneyan.com/tag/recsys/) RecSys blog\n",
    " - [ACM RecSys YouTube](https://www.youtube.com/channel/UC2nEn-yNA1BtdDNWziphPGA)\n",
    "\n",
    "**References:**\n",
    "\n",
    "- Jeremy Howard & Silvain Gugger. [*Deep Learning for Coders with fastai and PyTorch*](https://www.oreilly.com/library/view/deep-learning-for/9781492045519/). 2020. O'Reilly.\n",
    "- Kim Falk. [*Practical Recommender Systems*](https://www.manning.com/books/practical-recommender-systems). 2019. Manning.\n",
    "- [*Google Recommendation Systems Course*](https://developers.google.com/machine-learning/recommendation).\n",
    "\n",
    "\n",
    "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2024)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-ZUp8i37dFbU"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
