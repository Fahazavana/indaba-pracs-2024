{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Responsible AI**\n",
        "**Part 1: ProPublica's Analysis of the COMPAS Tool**\n",
        "\n",
        "**Part 2: Detecting and Mitigating Biases using Fairlearn**\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1W3nZF2AUsSbIKo_ekazO_rYDcPQSRzi8\" width=\"50%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1tWWxMkonHYbZi_J7wDTr2Zl2CCHKyZPD\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Â© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors:** Umang Bhatt and Kendall Brogle\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "This notebook provides a hands-on exploration of responsible AI through two parts: analyzing ProPublica's analysis of the COMPAS risk assessment tool and examining biases using the Fairlearn toolkit. The first part focuses ion ProPublica's investigation of COMPAS, specifically, on how its recidivism scores vary by race and sex. This involves data import, preprocessing, exploratory analysis, and logistic regression modeling to reproduce and interpret ProPublica's findings. The second part transitions to detecting and mitigating biases using Fairlearn, a library designed to assess and improve fairness in machine learning models. By engaging with both theoretical and practical aspects of responsible AI, this notebook aims to enhance understanding of bias in AI systems and the tools available to address\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: Responsible AI, Bias Detection and Mitigation, Logistic Regression, Fairness in AI models\n",
        "\n",
        "Level: Intermediate, Advanced\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "1) Understand and analyze bias in the COMPAS risk assessment tool.\n",
        "\n",
        "2) Apply logistic regression to explore racial bias in risk scores.\n",
        "\n",
        "3) Use the Fairlearn package to detect and mitigate biases in models.\n",
        "\n",
        "4) Evaluating fairness with metrics like demographic parity difference and ratio.\n",
        "\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "Basic understanding of machine learning concepts.\n",
        "\n",
        "Familiarity with Python and libraries like Pandas, Scikit-learn, and Matplotlib.\n",
        "\n",
        "Understanding of logistic regression and classification metrics.\n",
        "\n",
        "Familiarity with concepts of bias and fairness in AI.\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[Installation and Imports](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">[Part 1: ProPublica's Analysis of the COMPAS Tool](#scrollTo=G2sewZEq36T0)\n",
        "\n",
        ">[Part 2:  Detecting and Mitigating Biases using Fairlearn](#scrollTo=253jTpcO60Mf)\n",
        "\n",
        ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "Ensure all required Python packages are installed.\n",
        "\n",
        "Familiarize yourself with the dataset and variable descriptions.\n",
        "\n",
        "Review the key concepts of logistic regression and bias in AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Function to check for GPU/TPU and setup environment\n",
        "def check_accelerator():\n",
        "    try:\n",
        "        subprocess.check_output('nvidia-smi')\n",
        "        print(\"A GPU is connected.\")\n",
        "    except Exception:\n",
        "        # TPU or CPU\n",
        "        if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "            print(\"A TPU is connected.\")\n",
        "            import jax.tools.colab_tpu\n",
        "            jax.tools.colab_tpu.setup_tpu()\n",
        "        else:\n",
        "            print(\"Only CPU accelerator is connected.\")\n",
        "            # x8 cpu devices - number of (emulated) host devices\n",
        "            os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
        "\n",
        "check_accelerator()\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as skm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Import the data\n",
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "\n",
        "# For part 2\n",
        "# @markdown\n",
        "!pip install fairlearn folktables\n",
        "!git clone https://github.com/lurosenb/superquail\n",
        "\n",
        "from folktables import ACSDataSource, ACSEmployment, ACSIncome, ACSPublicCoverage, ACSTravelTime\n",
        "from superquail.data.acs_helper import ACSData\n",
        "from fairlearn.datasets import fetch_adult\n",
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, ErrorRate\n",
        "from fairlearn.postprocessing import ThresholdOptimizer\n",
        "from fairlearn.metrics import MetricFrame, demographic_parity_difference, demographic_parity_ratio, selection_rate, false_negative_rate, false_positive_rate\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xkOSgm_WjCVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: ProPublica's Analysis of the COMPAS Tool**"
      ],
      "metadata": {
        "id": "G2sewZEq36T0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "355LmyCRd_sZ"
      },
      "source": [
        "In 2016, [ProPublica published an analysis](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool. COMPAS is a proprietary tool which generates a so-called risk assessment for defendants in a criminal trial. ProPublica's analysis focused on the \"recidivism score,\" which purports to provide the likelihood of recidivism (i.e. committing a misdemeanor of felony) within two years of assessment.\n",
        "\n",
        "In this lab, we will go through parts of ProPublica's analysis of COMPAS, focusing on how the recidivism risk scale varies by race and sex.\n",
        "\n",
        "This section has four stages in which we will:\n",
        "1. Check the data, implement a few pre-processing steps, and inspect the data\n",
        "2. Run a short exploratory analysis of the COMPAS recidivism score, our primary variable of interest\n",
        "3. Reproduce the logistic regression model in ProPublica's analysis and interpret the estimates\n",
        "4. Compute the predictive accuracy of the risk score labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJDFA9XSwdMi"
      },
      "source": [
        "# Check Data\n",
        "\n",
        "Check the first few rows of data from ProPublica's compas-analysis repository on GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5B1icC-sbBg"
      },
      "outputs": [],
      "source": [
        "df_compas = pd.read_csv(url)\n",
        "print(\"Shape: \", df_compas.shape)\n",
        "df_compas.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI0slRqXFyXZ"
      },
      "source": [
        "## Notes on the Data\n",
        "\n",
        "Refer to the description of the [data collection methodology](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm). Salient points are highlighted below; see the full description from ProPublica for additional details.\n",
        "\n",
        "> **Goal:** We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period.\n",
        ">\n",
        "> **COMPAS tool input (data subjects):** When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of Risk of Recidivism and Risk of Violent Recidivism.\n",
        ">\n",
        "> **How COMPAS input was acquired by ProPublica:** Through a public records request, ProPublica obtained two years worth of COMPAS scores from the Broward County Sheriffâs Office in Florida. We received data for all 18,610 people who were scored in 2013 and 2014.\n",
        ">\n",
        "> **COMPAS tool output:** Each pretrial defendant received at least three COMPAS scores: \"Risk of Recidivism,\" \"Risk of Violence\" and \"Risk of Failure to Appear. [...] COMPAS scores for each defendant ranged from 1 to 10, with ten being the highest risk. Scores 1 to 4 were labeled by COMPAS as \"Low;\" 5 to 7 were labeled âMedium;\" and 8 to 10 were labeled âHigh.â\n",
        ">\n",
        "> **Data integration (record linkage):** Starting with the database of COMPAS scores, we built a profile of each personâs criminal history, both before and after they were scored. We collected public criminal records from the Broward County Clerkâs Office website through April 1, 2016. On average, defendants in our dataset were not incarcerated for 622.87 days (sd: 329.19). We matched the criminal records to the COMPAS records using a personâs first and last names and date of birth. This is the same technique used in the Broward County COMPAS validation study conducted by researchers at Florida State University in 2010. We downloaded around 80,000 criminal records from the Broward County Clerkâs Office website.\n",
        ">\n",
        "> **What is recidivism?** Northpointe defined recidivism as âa finger-printable arrest involving a charge and a filing for any uniform crime reporting (UCR) code.â We interpreted that to mean a criminal offense that resulted in a jail booking and took place after the crime for which the person was COMPAS scored. [...] For most of our analysis, we defined recidivism as a new arrest within two years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0xgPnT11OtO"
      },
      "source": [
        "# Inspect Data\n",
        "\n",
        "For convenience, here is a table of variable definitions:\n",
        "\n",
        "| Variable    | Description |\n",
        "| ----------- | ----------- |\n",
        "| age       |  Age of the defendant   |\n",
        "| age_cat   |  Age category. It can be < 25, 25-45, >45    |\n",
        "| sex   |  Sex of the defendant. It is either \"Male\" or \"Female\"       |\n",
        "| race   |  Race of the defendant. It can be \"African-American\", \"Caucasian\", \"Hispanic\", \"Asian\", or \"Other\"      |\n",
        "| c_charge_degree   |   Charge. Either \"M\" for misdemeanor, \"F\" for felony, or \"O\" (not causing jail time)    |\n",
        "| priors_count   |   Count of prior crimes committed by the defendant      |\n",
        "| days_b_screening_arrest   |  Days between the arrest and COMPAS screening       |\n",
        "| decile_score   |  The COMPAS score estimated by the system. It is between 0-10       |\n",
        "| score_text   |  Decile score. It can be \"Low\" (1-4), \"Medium\" (5-7), or \"High\" (8-10)       |\n",
        "| is_recid   |  Indicates if the defendant recidivated. It can be 0, 1, or -1      |\n",
        "| two_year_recid   |  Indicates if the defendant recidivated within two years of COMPAS assessment      |\n",
        "| c_jail_in   |   Date the defendant was in jail      |\n",
        "| c_jail_out  |   Date when the defendant was released from jail     |\n",
        "\n",
        "\\\n",
        "# **TODO 1** Plot the distribution of age, race, and sex in the imported data (```df_compas```):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:\n"
      ],
      "metadata": {
        "id": "ha4MebGr5Jd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6lQj5HJVuyw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Answer\n",
        "df_compas[\"age\"].hist()\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "df_compas[\"race\"].value_counts().plot(kind = \"bar\")\n",
        "plt.xlabel(\"Race\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "df_compas[\"sex\"].value_counts().plot(kind = \"bar\")\n",
        "plt.xlabel(\"Sex\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q36OxXLfoJxt"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYL5nnxko0bG"
      },
      "source": [
        "ProPublica implemented a few pre-processing steps. First, they generated a subset of the data with a few variables of interest. Here, we select even fewer variables, keeping only those that we will use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1qppbDjoRGJ"
      },
      "outputs": [],
      "source": [
        "cols_to_keep = [\"id\", \"age\", \"c_charge_degree\", \"race\", \"age_cat\", \"score_text\",\n",
        "                \"sex\", \"priors_count\", \"days_b_screening_arrest\",\n",
        "                \"decile_score\", \"is_recid\", \"two_year_recid\"]\n",
        "\n",
        "df_selected = df_compas[cols_to_keep].copy()\n",
        "\n",
        "print(\"Shape: \", df_selected.shape)\n",
        "df_selected.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZE7FZUpBsQ"
      },
      "source": [
        "Take a moment to get a feel for the variables and structure of the data. ProPublica filtered the above data by removing rows where:\n",
        "\n",
        "1. The COMPAS score is missing.\n",
        "1. The charge date of the defendant's COMPAS-scored crime was not within 30 days from the date of arrest. ProPublica assumed that the offense may not be correct in these cases.\n",
        "2. The recividist flag is \"-1\". In such cases, ProPublica could not find a COMPAS record at all.\n",
        "3. The charge is \"O\". These are ordinary traffic offenses and do not result in jail time.\n",
        "\n",
        "We implement these conditions here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3muereQrap8u"
      },
      "outputs": [],
      "source": [
        "df_analysis = df_selected[\n",
        "    (df_selected.score_text != \"N/A\") &\n",
        "    (df_selected.days_b_screening_arrest <= 30) &\n",
        "    (df_selected.days_b_screening_arrest >= -30) &\n",
        "    (df_selected.is_recid != -1) &\n",
        "    (df_selected.c_charge_degree != \"O\")\n",
        "    ].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCNlGWB_sBda"
      },
      "source": [
        "Note that ProPublica only included people who had recidivated within two years or had at least two years outside a correctional facility. This pre-processing step is \"baked in\" to the data that we imported from GitHub in this notebook.\n",
        "\n",
        "# **TODO 2** Check the dimensions (i.e. the number of variables and observations) of the imported (```df_compas```) and preprocessed (```df_analysis```) data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "xPYaEmwW5lse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ_MkWKxt1YJ",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"Imported data\", df_compas.shape)\n",
        "print(\"Data after selecting variables\", df_selected.shape)\n",
        "print(\"Data after filtering observations\", df_analysis.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0juptQDv7pDG"
      },
      "source": [
        "Take the additional step of making sure that the decile score (discussed below) is numeric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjg2B1s47tJr"
      },
      "outputs": [],
      "source": [
        "df_analysis[\"decile_score\"] = pd.to_numeric(df_analysis[\"decile_score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS1ZlkXTwTrU"
      },
      "source": [
        "# Inspect Data Again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bgG7Odrk1Wf"
      },
      "source": [
        "# **TODO 3** Re-inspect salient variables in the data after the preprocessing steps. Plot the distribution of age, race, and sex in the preprocessed data (```df_analysis```) and compare these distributions to the imported data (```df_compas```):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "xT0Knt1a5u0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf4Jdy6QwRAS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "plt.hist(df_compas[\"age\"], alpha = 0.5, label = \"imported\")\n",
        "plt.hist(df_analysis[\"age\"], alpha = 0.5, label= \"filtered\")\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "df_compas[\"race\"].value_counts().plot(kind = \"bar\", alpha=0.5)\n",
        "df_analysis[\"race\"].value_counts().plot(kind = \"bar\", alpha=0.5, color='orange')\n",
        "plt.xlabel(\"Race\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "df_compas[\"sex\"].value_counts().plot(kind = \"bar\", alpha=0.5)\n",
        "df_analysis[\"sex\"].value_counts().plot(kind = \"bar\", color='orange', alpha=0.5)\n",
        "plt.xlabel(\"Sex\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnlEcgYrwIkw"
      },
      "source": [
        "# **TODO 4** Observe that we are iterating through the data analysis: import, inspect & profile, preprocess, and profile again. Generate a crosstab summarizing the number of observations by race and sex:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "wwhuArp05yNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQnZbZDF162n",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "df_analysis.pivot_table(values = [\"id\"], columns = [\"race\"],\n",
        "                        index = \"sex\", aggfunc = lambda x: len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxkyKFMP3Oxa"
      },
      "source": [
        "# Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9a0-JeX3Wmd"
      },
      "source": [
        "Let's turn our focus to the primary variable of interest: the COMPAS recidivism score. In this exploratory analysis, we are interested in the variable named \"decile_score\".\n",
        "\n",
        "The ProPublica analysis notes: \"Judges are often presented with two sets of scores from the COMPAS system: one that classifies people into high, medium or low risk, and a corresponding decile score.\"\n",
        "\n",
        "Plot the distribution of decile score for males and for females. To what extent do these distributions differ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ntoFh0w-1F3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# plot score decile by sex\n",
        "df_female = df_analysis[(df_analysis.sex == \"Female\")].copy()\n",
        "df_male   = df_analysis[(df_analysis.sex == \"Male\")].copy()\n",
        "\n",
        "fig = plt.figure(figsize = (12, 6))\n",
        "fig.add_subplot(121)\n",
        "\n",
        "plt.hist(df_female[\"decile_score\"], ec = \"white\",\n",
        "         weights = np.ones(len(df_female[\"decile_score\"])) /\n",
        "         len(df_female[\"decile_score\"]))\n",
        "plt.xlabel(\"Decile Score (0-10)\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"Female Defendants' Decile Scores\")\n",
        "plt.ylim([0, 0.25])\n",
        "\n",
        "fig.add_subplot(122)\n",
        "plt.hist(df_male[\"decile_score\"], ec = \"white\",\n",
        "         weights = np.ones(len(df_male[\"decile_score\"])) /\n",
        "         len(df_male[\"decile_score\"]))\n",
        "plt.xlabel(\"Decile Score (0-10)\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"Male Defendants' Decile Scores\")\n",
        "plt.ylim([0, 0.25])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDtwnIWk4O_P"
      },
      "source": [
        "# **TODO 5** What about race? Repeat the above plots for Black defendants and White defendants:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "m_Ea1Wdu55jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QRK2jsM3Vvn",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "\n",
        "# plot score decile by race\n",
        "df_black = df_analysis[(df_analysis.race == \"African-American\")]\n",
        "df_white = df_analysis[(df_analysis.race == \"Caucasian\")]\n",
        "\n",
        "fig = plt.figure(figsize = (12, 6))\n",
        "fig.add_subplot(121)\n",
        "\n",
        "plt.hist(df_black[\"decile_score\"], ec = \"white\",\n",
        "         weights = np.ones(len(df_black[\"decile_score\"])) /\n",
        "         len(df_black[\"decile_score\"]))\n",
        "plt.xlabel(\"Decile Score (0-10)\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"Black Defendants' Decile Scores\")\n",
        "plt.ylim([0, 0.30])\n",
        "\n",
        "fig.add_subplot(122)\n",
        "plt.hist(df_white[\"decile_score\"], ec = \"white\",\n",
        "         weights = np.ones(len(df_white[\"decile_score\"])) /\n",
        "         len(df_white[\"decile_score\"]))\n",
        "plt.xlabel(\"Decile Score (0-10)\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"White Defendants' Decile Scores\")\n",
        "plt.ylim([0, 0.30])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v4KvrqCNaqi"
      },
      "source": [
        "# **TODO 6** Summarize the difference between the distribution of decile scores for Black defendants and White defendants (three sentences maximum):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "zbsAvmZ457KP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1o9A7VlDWuo"
      },
      "source": [
        "# **TODO 7** Plot the distribution of COMPAS-assigned \"risk labels\" (the variable is named \"score_text\") for Black defendants and White defendants:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "c-hUKvFj59eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3tnCwrbDvIe",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "\n",
        "# compute risk group by race\n",
        "fig = plt.figure(figsize = (12, 6))\n",
        "\n",
        "fig.add_subplot(121)\n",
        "(df_black[\"score_text\"].value_counts().reindex(['Low', 'Medium', 'High']) /\n",
        "    len(df_black)).plot(kind = \"bar\")\n",
        "plt.xlabel(\"Score Label\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"Score Split for Black Defendants\")\n",
        "plt.ylim([0, .7])\n",
        "\n",
        "fig.add_subplot(122)\n",
        "(df_white[\"score_text\"].value_counts().reindex(['Low', 'Medium', 'High']) /\n",
        "    len(df_white)).plot(kind = \"bar\")\n",
        "plt.xlabel(\"Score Label\")\n",
        "plt.ylabel(\"Percent of Cases\")\n",
        "plt.title(\"Score Split for White Defendants\")\n",
        "plt.ylim([0, .7])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfFXAqvNFFXi"
      },
      "source": [
        "# Bias in COMPAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcvC8rNFbYc"
      },
      "source": [
        "ProPublica focused on racial bias in the COMPAS algorithm. In general terms, ProPublica analyzed (i) how the *risk scores* vary by race and (ii) the extent to which the *risk labels* assigned to defendants matches up with their observed recidivism and how this varies by race. We will (approximately) reproduce this analysis below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOCIMREOQ7I0"
      },
      "source": [
        "## Preprocess Data for Logistic Regression\n",
        "\n",
        "ProPublica used a logistic regression model to analyze variation in the risk scores by race. We will prepare the data by one-hot encoding the categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqJLl5RfFIXR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(df_analysis.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VywniiqFugA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for i, col_type in enumerate(df_analysis.dtypes):\n",
        "    if col_type == \"object\":\n",
        "        print(\"\\nVariable {} takes the values: {}\".format(\n",
        "            df_analysis.columns[i],\n",
        "            df_analysis[df_analysis.columns[i]].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbPi5iRYHZ64"
      },
      "outputs": [],
      "source": [
        "df_logistic = df_analysis.copy()\n",
        "\n",
        "# one-hot encoding\n",
        "df_logistic = pd.get_dummies(df_logistic,\n",
        "                             columns = [\"c_charge_degree\", \"race\",\n",
        "                                        \"age_cat\", \"sex\"])\n",
        "\n",
        "# mutate score_text to binary variable where low = {low}\n",
        "# and high = {medium, high}\n",
        "df_logistic[\"score_binary\"] = np.where(df_logistic[\"score_text\"] != \"Low\",\n",
        "                                       \"High\", \"Low\")\n",
        "df_logistic[\"score_binary\"] = df_logistic[\"score_binary\"].astype('category')\n",
        "\n",
        "# rename the columns to be more instructive and consistent with statsmodel\n",
        "# requirements for variable names\n",
        "df_logistic.columns = df_logistic.columns.str.replace(' ', '_')\n",
        "df_logistic.columns = df_logistic.columns.str.replace('-', '_')\n",
        "\n",
        "renamed_cols = {'age_cat_25___45':'age_cat_25_to_45',\n",
        "                'c_charge_degree_F':'Felony',\n",
        "                'c_charge_degree_M':'Misdemeanor'}\n",
        "\n",
        "df_logistic = df_logistic.rename(columns = renamed_cols)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BaOl-bMfU3q"
      },
      "source": [
        "Check that recoding resulted in the desired data structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rAihNgrP3f7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df_logistic.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NKIjUEgRHX6"
      },
      "source": [
        "## Estimate Logistic Regression Model\n",
        "\n",
        "Following ProPublica, we specify the following logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd07D4i_MUjN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Right-hand side\n",
        "explanatory = \"priors_count + two_year_recid + Misdemeanor + \\\n",
        "age_cat_Greater_than_45 + age_cat_Less_than_25 + \\\n",
        "race_African_American + race_Asian + race_Hispanic + race_Native_American + \\\n",
        "race_Other + sex_Female\"\n",
        "\n",
        "# Left-hand side\n",
        "response = \"score_binary\"\n",
        "\n",
        "# Formula\n",
        "formula = response + \" ~ \" + explanatory\n",
        "print(formula)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Gk-XzAfz0d"
      },
      "source": [
        "Let's fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXGOejPiNk3E",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Note: using family = sm.families.Binomial() specifies a logistic regression\n",
        "model = sm.formula.glm(formula = formula,\n",
        "                       family = sm.families.Binomial(),\n",
        "                       data = df_logistic).fit()\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ7CaxsERKus"
      },
      "source": [
        "## Interpret Estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOE5Yi5dR6rG"
      },
      "source": [
        "Take a moment to read through the model summary.\n",
        "\n",
        "One way to interpret the estimates is by calculating odds ratios. To calculate odds ratios, we take the exponential of the coefficients. For example, taking the exponential of the coefficient for sex_Female ($\\beta_{female}$ = 0.2213) will return the odds of score_text taking the value \"high\" for a female relative to a male.\n",
        "\n",
        "# **TODO 8** Calculate this odds ratio here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "ILqN3aes6HoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-kJeX2TSu29",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "np.exp(0.2213)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt01yUzKTzG_"
      },
      "source": [
        "In words, the odds that COMPAS labeled a defendant as \"high risk\" of recidivism is 1.25 times greater for a female than a male.\n",
        "\n",
        "# **TODO 9** Calculate the odds ratio for all of the coefficients in the model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "KKjqygPZ6It2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nPb8HKJROFd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "np.exp(model.params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy_qGPaYj-r"
      },
      "source": [
        "Take a moment to read through these coefficients. What is the reference category for each variable? (e.g. For females, the reference category is male.) Think in terms of comparisons, for example:\n",
        "\n",
        "> A person with a value of [ &nbsp; &nbsp; ] on variable [ &nbsp; &nbsp; ] is [ &nbsp; &nbsp; ] times more likely to be labeled high risk compared to a person with a value of [ &nbsp; &nbsp; ] on variable [ &nbsp; &nbsp; ]\n",
        "\n",
        "In the female example above, this could be stated:\n",
        "\n",
        "> \"A person with a value of female on variable sex is 1.25 times more likely to be labeled high risk compared to a person with a value of male on variable sex\"\n",
        "\n",
        "Of course, we should be more straightforward when writing up results. \"A person with a value of male on variable sex\" is rather verbose; \"males\" will suffice. Interpreting model estimates in straightforward terms is an underrated skill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dQBzQ5jSjPy"
      },
      "source": [
        "# **TODO 10** Summarize the odds associated with the \"age_cat\" variable (two sentences maximum):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "6VS_hKG-6J1W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvGGSeCqY-eN"
      },
      "source": [
        "## Predictive Accuracy\n",
        "\n",
        "In terms of fairness, ProPublica focused on the predictive accuracy of the COMPAS algorithm. In this case, predictive accuracy refers to the concordance between a person's recidivism and the label assigned to that person by the COMPAS algorithm. For instance, how often did COMPAS predict that a person was at \"high risk\" of recidivism and that person in fact recidivated within two years? We can think of this in terms of a 2x2 table:\n",
        "\n",
        "|      | Did not recidivate | Recidivated   |\n",
        "| :---        |    :----:   |          ---: |\n",
        "| **Labeled high risk**  | A       | B   |\n",
        "| **Labeled low risk**   | C       | D      |\n",
        "\n",
        "ProPublica reported A and D for black defendants and white defendants, separately.\n",
        "\n",
        "# **TODO 11** What are generic terms for A and D? Why focus on A and D?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "4yYTiIE96ffY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5zVwL9M6yzk"
      },
      "source": [
        "ProPublica used a somewhat different data set to calculate the predictive accuracy of COMPAS. In this section we will use the ```df_logistic``` data we preprocessed above for brevity. Note therefore that the numbers we calculate below will not match those reported by ProPublica. Let's generate a crosstab of the variable denoting recidivism within two years (```is_recid```) and the binary score variable (```score_binary```):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpluSwppZFUe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"All defendants\")\n",
        "pd.crosstab(df_logistic[\"score_binary\"], df_logistic[\"is_recid\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kojOZPLybCSv"
      },
      "source": [
        "# **TODO 12** Based on this crosstab, input the number of true positives, false positives, true negatives, and false negatives:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Av4Eu0ZjFN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "true_positive  = 1817#@param {type:\"number\"}\n",
        "false_positive = 934#@param {type:\"number\"}\n",
        "true_negative  = 2248#@param {type:\"number\"}\n",
        "false_negative = 1173#@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOKUOXBh6rh0"
      },
      "source": [
        "You can calculate the false positive rate by taking FP / (FP + TN), where FP is the number of false positives and TN is the number of true negatives. Calculate the false positive rate:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "8uZS5nvV6k_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrkCFBzAZa2R",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"All defendants\")\n",
        "print(\"False positive rate\",\n",
        "      false_positive / (false_positive + true_negative) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgmNak0ob2C8"
      },
      "source": [
        "# **TODO 13** Now calculate the false *negative* rate: (hint, replace the terms in the false positive rate formula in the previous text cell)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "r-MvPI4A6mgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46bwGfLba3ij",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"All defendants\")\n",
        "print(\"False negative rate\",\n",
        "      false_negative / (false_negative + true_positive) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1LBhe-f-TTn"
      },
      "source": [
        "# **TODO 14** How do the false positive and false negative rates vary by sex? Let's generate a crosstab of \"score_binary\" and \"is_recid\" for female defendants and calculate the false positive and false negative rates for females:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "2HmdqzAC6obN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI3i9zzt-kMG",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "mask = df_logistic[\"sex_Female\"] == 1\n",
        "print(pd.crosstab(df_logistic.loc[mask, \"score_binary\"],\n",
        "                  df_logistic.loc[mask, \"is_recid\"]))\n",
        "print(\"Female defendants\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tzBrFZKb4UI",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "tp = 256\n",
        "fp = 220\n",
        "tn = 520\n",
        "fn = 179\n",
        "print(\"False positive rate\", fp / (fp + tn) * 100)\n",
        "print(\"False negative rate\", fn / (fn + tp) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYowpvPWcooI"
      },
      "source": [
        "# **TODO 15** Now calculate the false positive and false negative rates for male defendants:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "yF3fHREC6tHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlxlpA9SARFT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "mask = df_logistic[\"sex_Male\"] == 1\n",
        "print(pd.crosstab(df_logistic.loc[mask, \"score_binary\"],\n",
        "                  df_logistic.loc[mask, \"is_recid\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yexu8ZUIb78V",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"Male defendants\")\n",
        "tp = 1561\n",
        "fp = 714\n",
        "tn = 1728\n",
        "fn = 994\n",
        "print(\"False positive rate\", fp / (fp + tn) * 100)\n",
        "print(\"False negative rate\", fn / (fn + tp) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y3odoqE-g0y"
      },
      "source": [
        "# **TODO 16** How do the false positive and false negative rates vary by race? Calculate the false positive rate and false negative rate for White defendants:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "zT1YJCJJ6uVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0-offlbBQTy",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "mask = df_logistic[\"race_Caucasian\"] == 1\n",
        "print(pd.crosstab(df_logistic.loc[mask, \"score_binary\"],\n",
        "                  df_logistic.loc[mask, \"is_recid\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ3qx2aGcGxm",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"White defendants\")\n",
        "tp = 430\n",
        "fp = 266\n",
        "tn = 963\n",
        "fn = 444\n",
        "print(\"False positive rate\", fp / (fp + tn) * 100)\n",
        "print(\"False negative rate\", fn / (fn + tp) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU0ft0TSe_1m"
      },
      "source": [
        "# **TODO 17** Lastly, calculate the false positive rate and false negative rate for Black defendants:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "gwMqwCli6vOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUU_w57EfEDk",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "mask = df_logistic[\"race_African_American\"] == 1\n",
        "print(pd.crosstab(df_logistic.loc[mask, \"score_binary\"],\n",
        "                  df_logistic.loc[mask, \"is_recid\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kasSRZjkcYET",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "print(\"Black defendants\")\n",
        "tp = 1248\n",
        "fp = 581\n",
        "tn = 821\n",
        "fn = 525\n",
        "print(\"False positive rate\", fp / (fp + tn) * 100)\n",
        "print(\"False negative rate\", fn / (fn + tp) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253jTpcO60Mf"
      },
      "source": [
        "# **Part 2: Detecting and Mitigating Biases using Fairlearn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LrSVlCSDA9Z"
      },
      "source": [
        "#Detecting Bias Using Fairlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuBQtfv5iZac"
      },
      "source": [
        "## Bias in ML\n",
        "\n",
        "A machine learning algorithm will attempt to find patterns, or generalizations, in the training dataset to use when a prediction for a new instance is needed. For example, the model may discover a pattern whereby a person with a salary over \\$40,000 and an outstanding debt of less than $5 is very likely to repay a loan.\n",
        "\n",
        "However, sometimes the patterns that are found and replicated by a model may not be desirable or, even worse, might be illegal. For example, a loan repayment model may determine that age plays a significant role in the prediction of repayment because the training dataset happened to have better repayment for one age group compared to another. This raises two problems: 1) the training dataset may not be representative of the true population of loan applications for all age groups, and 2) even if it is representative, it is illegal (with limited exceptions) to base loan decisions on an applicant's age, regardless of whether this is an accurate basis for prediction based on historical data.\n",
        "\n",
        "The loan scenario describes an intuitive example of illegal bias. However, not all undesirable biases in machine learning are illegal; it may also exist in more subtle ways. For example, a loan company may want a diverse portfolio of customers across all income levels, and thus, will deem it undesirable if they are making more loans to high income levels over low income levels. Although this is not illegal or unethical, it is undesirable for the company's strategy.\n",
        "\n",
        "## The `Fairlearn` toolkit\n",
        "\n",
        "Fairlearn is a toolkit designed to help address this problem with fairness metrics and bias mitigators. Fairness metrics can be used to check for bias in machine learning workflows. Bias mitigators can be used to overcome bias in the workflow to produce a more fair outcome.\n",
        "\n",
        "As these two examples illustrate, a bias detection and/or mitigation toolkit needs to be tailored to the particular bias of interest. More specifically, we need to define the attribute(s), called protected (or sensitive) attributes of interest: the attribute whose skewness/bias we are trying to detect and mitigate. The term suggests that the system designer should be sensitive to these features when assessing and mitigating group fairness.\n",
        "\n",
        "Several stages of the machine learning pipeline are susceptible to bias. One useful way to categorize these stages are, intuitively, 'before,' 'during,' and 'after' training a model. These stages are commonly referred to as *pre-processing*, *in-processing*, and *post-processing* (in Fairlearn, in-processing techniques are available in the *reductions* package)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip6sQt0eQLAU"
      },
      "source": [
        "## Fairlearn\n",
        "\n",
        "In part 2 we will use Fairlearn to detect and mitigate bias in a classifier. We will use the [ACS PUMS files](https://www.census.gov/programs-surveys/acs/microdata.html), particularly a fraction of the ACS Income dataset, and train a classifier to predict whether an individual has a salary greater than $50K. The protected attribute will be the sex of the individual.\n",
        "\n",
        "In part 2, we will:\n",
        "\n",
        "1. Explore possible fairness metrics\n",
        "2. Train a logistic regression classifier and evaluate the fairness of this classifier\n",
        "3. Train other logistic regression classifiers with pre-processing interventions and re-evaluate fairness\n",
        "4. Compare the results obtained in 2 and 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XttcS7t0SNbL"
      },
      "source": [
        "#1. Load Data, Conduct Exploratory Analysis, and Preprocess Data\n",
        "Next, we will load the Folktables dataset.  The Folktables dataset is taken from US Census Data and is built to solve a few simple prediction tasks. The sample we pull is data from 2018 in California. The column names are described in the table below. Note that certain categorical variables have been mapped to integer values, which we will keep as-is for the following analyses.\n",
        "\n",
        "For more information on the this dataset, please see the following paper (namely page 18): https://eaamo2021.eaamo.org/accepted/acceptednonarchival/EAMO21_paper_16.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErJSui-veHGd"
      },
      "source": [
        "| Column Name | Feature | Description/Notes |\n",
        "| --- | ----------- | --- |\n",
        "| PINCP | Total personâs income | (Target) 1 if >= $50k, 0 if less |\n",
        "| SEX | Sex | (Sensitive Attribute) Male=1, Female=2 |\n",
        "| RAC1P | Race | (Sensitive Attribute) White=1, Black=2, Other races are between 3 and 9 |\n",
        "| AGEP | Age | Ranges from 0-99 |\n",
        "| COW | Class of Worker | Ranges 1-9, see paper for description |\n",
        "| SCHL | Education Level | Ranges 1-24, see paper for description |\n",
        "| MAR | Marital Status | Ranges 1-5, see paper for description |\n",
        "| OCCP | Occupation | Codes taken from Public Use Microdata Sample (PUMS) from the US Census, see paper |\n",
        "| POBP | Place of Birth | Codes taken from Public Use Microdata Sample (PUMS) from the US Census, see paper |\n",
        "| RELP | Relationship | Relationship of individual to person who responded to the Census taker. Ranges 0-17, see paper for description |\n",
        "| WKHP | Hours worked per week | Ranges from 0-99, averaged over previous year |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgVUEi3nET1p",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Read in the folktables dataset\n",
        "full_df, features_df, target_df, groups_df = ACSData().return_acs_data_scenario(scenario=\"ACSIncome\", subsample=70000)\n",
        "\n",
        "print(full_df.shape)\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D-hmorRFKia",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Check missing values and data types\n",
        "full_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1x2mdUaID9G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Change data types of categorical features\n",
        "numerical_cols = ['AGEP','WKHP']\n",
        "categorical_cols = ['COW','SCHL','MAR','OCCP','POBP','RELP','RAC1P','SEX']\n",
        "\n",
        "for col in categorical_cols:\n",
        "  full_df[col] = full_df[col].astype('int')\n",
        "  full_df[col] = full_df[col].astype('str')\n",
        "\n",
        "full_df.info()\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHRpGsboZ_wa"
      },
      "source": [
        "Next, we will conduct some basic exploratory analysis of the data beginning with plotting the distributions of our features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umsO2aZ3J3WT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Plot distribution of categorical columns\n",
        "fig, ax = plt.subplots(2,4,figsize=(12,8))\n",
        "ax[0,0].barh(full_df['COW'].value_counts().index[::-1], full_df['COW'].value_counts()[::-1])\n",
        "ax[0,0].set_title('COW')\n",
        "\n",
        "ax[0,1].barh(full_df['SCHL'].value_counts().index[:10][::-1], full_df['SCHL'].value_counts()[:10][::-1])\n",
        "ax[0,1].set_title('SCHL (top-10)')\n",
        "\n",
        "ax[0,2].barh(full_df['MAR'].value_counts().index[::-1], full_df['MAR'].value_counts()[::-1])\n",
        "ax[0,2].set_title('MAR')\n",
        "\n",
        "ax[0,3].barh(full_df['OCCP'].value_counts().index[:10][::-1], full_df['OCCP'].value_counts()[:10][::-1])\n",
        "ax[0,3].set_title('OCCP (top-10)')\n",
        "\n",
        "ax[1,0].barh(full_df['POBP'].value_counts().index[:10][::-1], full_df['POBP'].value_counts()[:10][::-1])\n",
        "ax[1,0].set_title('POBP (top-10)')\n",
        "\n",
        "ax[1,1].barh(full_df['RELP'].value_counts().index[:10][::-1], full_df['RELP'].value_counts()[:10][::-1])\n",
        "ax[1,1].set_title('RELP (top-10)')\n",
        "\n",
        "ax[1,2].barh(full_df['SEX'].value_counts().index[::-1], full_df['SEX'].value_counts()[::-1])\n",
        "ax[1,2].set_title('SEX')\n",
        "labels = ('Female = 2', 'Male = 1')\n",
        "ax[1,2].set_yticklabels(labels)\n",
        "\n",
        "ax[1,3].barh(full_df['RAC1P'].value_counts().index[::-1], full_df['RAC1P'].value_counts()[::-1])\n",
        "ax[1,3].set_title('RAC1P')\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIN5VbbVQT4_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Plot distribution of numeric features\n",
        "fig, ax = plt.subplots(1,2,figsize=(12,8))\n",
        "\n",
        "#histogram for AGEP (age)\n",
        "num_of_bins_agep = 10\n",
        "y_vals_agep, x_vals_agep, e_agep = ax[0].hist(full_df['AGEP'], bins=num_of_bins_agep, edgecolor='black')\n",
        "ax[0].set_title(\"Histogram of AGEP\")\n",
        "ax[0].set_xlabel(\"age\")\n",
        "ax[0].set_ylabel(\"Percentage\")\n",
        "y_max_agep = round((max(y_vals_agep) / len(full_df)) + 0.02, 2)\n",
        "ax[0].set_yticks(ticks=np.arange(0.0, y_max_agep * len(full_df), 0.01 * len(full_df)))\n",
        "ax[0].set_ylim(ax[0].get_yticks()[0], ax[0].get_yticks()[-1])\n",
        "ax[0].yaxis.set_major_formatter(ticker.PercentFormatter(xmax=len(full_df)))\n",
        "\n",
        "#histogram for WKHP (Hours worked per week)\n",
        "num_of_bins_wkhp = 10\n",
        "y_vals_wkhp, x_vals_wkhp, e_wkhp = ax[1].hist(full_df['WKHP'], bins=num_of_bins_wkhp, edgecolor='black')\n",
        "ax[1].set_title(\"Histogram of WKHP\")\n",
        "ax[1].set_xlabel(\"hours worked per week\")\n",
        "ax[1].set_ylabel(\"Percentage\")\n",
        "y_max_wkhp = round((max(y_vals_wkhp) / len(full_df)) + 0.05, 2)\n",
        "ax[1].set_yticks(ticks=np.arange(0.0, y_max_wkhp * len(full_df), 0.05 * len(full_df)))\n",
        "ax[1].set_ylim(ax[1].get_yticks()[0], ax[1].get_yticks()[-1])\n",
        "ax[1].yaxis.set_major_formatter(ticker.PercentFormatter(xmax=len(full_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhrMJoM1sOrj"
      },
      "source": [
        "As we can see, there is nothing very unusual about the distritubtion of the features in this dataset. Also, we note that the proportion of men and women is rather balanced.\n",
        "\n",
        "We can also examine the pairwise correlations between the numeric features and our target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXO7X2tMyTI6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Plot pairwise correlations between numeric features\n",
        "\n",
        "sns.heatmap(full_df.corr(), mask=np.identity(len(full_df.corr())), annot=True, cmap='Blues')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kyJCKQ3yxhQ"
      },
      "source": [
        "Here, we can see that there are only relatively low correlations between our target variable (PINCP) and our numeric features of age and hours worked per week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL6p5-Axxbvh"
      },
      "source": [
        "Next, we can look at the distribution of our target variable as well as the joint distribution of our protected and target attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JCRb5o76oiH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Examine distribution of target variable\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "full_df['PINCP'].value_counts().divide(full_df.shape[0]).plot(kind='bar')\n",
        "ax.set_xlabel('Income')\n",
        "ax.set_ylabel('Frequency')\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha='center')\n",
        "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
        "labels_target = ('0 (<$50k)', '1 (>=$50k)')\n",
        "ax.set_xticklabels(labels_target)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq9_1ZSXduNs"
      },
      "source": [
        "From the graph above we note that there is a considerable imbalance of the target variable. Let's see how is this distribution looks by gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWqWUOfamsAm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Plot distribution of target variable among males and plot distribution of target variable among females\n",
        "hist_df = full_df.groupby(['SEX','PINCP']).size().to_frame('count').reset_index()\n",
        "new_col = full_df.groupby(['SEX']).PINCP.value_counts(normalize=True).values\n",
        "hist_df['frac'] = new_col\n",
        "hist_df.replace({'SEX': {'1': 'Male', '2': 'Female'}}, inplace=True)\n",
        "hist_df.replace({'PINCP': {0.0: '<$50k', 1.0: '>=$50k'}}, inplace=True)\n",
        "sns.barplot(x='PINCP', y='frac', hue='SEX', data=hist_df)\n",
        "plt.ylabel('Percent of Group')\n",
        "plt.xlabel('Income')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb4qHH0dQYAB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Showing the number of Males and Females per Income group\n",
        "full_df.groupby(['SEX', 'PINCP'])['PINCP'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lB_TQJGE1tV"
      },
      "source": [
        "Here, we can see that the proportion of males who make at least \\$50k is higher than the proportion of females who make at least $50k in this dataset. Thus, the initial finding about imbalance of the target variable is more accentuated for females."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ffXieBGVGx"
      },
      "source": [
        "# **TODO 1**: Given the graphs above, what results might we expect from our classifier when it comes to labelling males and females as high or low income?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "P62gK8l07Hh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "'''Since the proportion of males in this dataset that make at least $50k is higher than the proportion of females in this dataset\n",
        "that make at least $50k, we might expect our classifier to have a bias for labelling males as higher income than females.'''"
      ],
      "metadata": {
        "id": "7p2T0SGz9hVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- **Answer:** Since the proportion of males in this dataset that make at least \\$50k is higher than the proportion of females in this dataset that make at least \\$50k, we might expect our classifier to have a bias for labelling males as higher income than females. -->"
      ],
      "metadata": {
        "id": "Z1-hPuYr-vV1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vccos4UWJieU"
      },
      "source": [
        "# **TODO 2**: Why might this data be biased? What type of bias is this?\n",
        "\n",
        "<!-- **Answer:** Males might be more inclined than females to inflate reports of their actual earnings.  Additionally, there are documented, historic discrepancies in compensation between males and females for similar work due to a variety reasons (discrepancies in education, labor market etc.). Therefore, the differences in income in this dataset could be reflections of a real-world wage-gap.  Both of these would be examples of \"pre-existing bias\" in this dataset. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "ntofxi1T7JTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "'''Males might be more inclined than females to inflate reports of their actual earnings.  Additionally, there are documented,\n",
        "historic discrepancies in compensation between males and females for similar work due to a variety reasons (discrepancies in\n",
        "education,labor market etc.). Therefore, the differences in income in this dataset could be reflections of a real-world\n",
        "wage-gap. Both of these would be examples of \"pre-existing bias\" in this dataset.'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B4TMUf_9_4Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TODO 3**: Write some code that replicates the above histogram plot for black and white individuals in the data."
      ],
      "metadata": {
        "id": "khnT4aDn7Pfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your work here:"
      ],
      "metadata": {
        "id": "Tc4N54Sq7eHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQP7PQb0BGWv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "hist_df = full_df.groupby(['RAC1P','PINCP']).size().to_frame('count').reset_index()\n",
        "new_col = full_df.groupby(['RAC1P']).PINCP.value_counts(normalize=True).values\n",
        "hist_df['frac'] = new_col\n",
        "hist_df['RAC1P'] = hist_df['RAC1P'].map({'1': 'White', '2': 'Black'})#.fillna('Other')\n",
        "hist_df.replace({'PINCP': {0.0: '<$50k', 1.0: '>=$50k'}}, inplace=True)\n",
        "sns.barplot(x='PINCP', y='frac', hue='RAC1P', data=hist_df)\n",
        "plt.ylabel('Percent of Group')\n",
        "plt.xlabel('Income')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7mYHwvvrlZn"
      },
      "source": [
        "## Preprocessing\n",
        "Next, we will do some preprocessing on our data to prepare it for use in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJmRpooUU6cC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# tandardize numerical features\n",
        "scaler = StandardScaler()\n",
        "full_df[numerical_cols] = scaler.fit_transform(full_df[numerical_cols])\n",
        "display(full_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttOb_ULRsGUB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# One-hot encode categorical features\n",
        "full_df = pd.get_dummies(full_df, columns=categorical_cols)\n",
        "display(full_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rLw5F6_sHew",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Since the sex attribute is already binary we can drop one of the redundant dummy columns\n",
        "#note: males are now labeled as 1 and females are labeled as 0\n",
        "full_df.drop(columns=['SEX_2'], inplace=True)\n",
        "full_df.rename(columns={'SEX_1':'SEX'}, inplace=True)\n",
        "\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lldcQFkCx1Y5"
      },
      "source": [
        "#2. Train Logistic Regression Classifier\n",
        "Next, we will split our data into training and test sets randomly.  Then, we will train a logistic regression classifier and evaluate possible biases within that classifier.\n",
        "\n",
        "### Aside: Accuracy as a Metric\n",
        "\n",
        "Traditional machine learning (i.e. without a focus on fairness) often measures the quality of a classifier by it's **accuracy**, or what fraction of samples were labeled correctly:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of Correctly Labeled People}}{\\text{Total Number of People}}\n",
        "$$\n",
        "This can also be expressed using the terms of the [\"Confusion Matrix\"](https://en.wikipedia.org/wiki/Confusion_matrix), where we let\n",
        "- $\\text{TP} = $ \"True Positives\" $ = \\text{Num. people Correctly labeled as Positive}$\n",
        "- $\\text{FP} = $ \"False Positives\" $ = \\text{Num. people Wrongly labeled as Positive}$\n",
        "- $\\text{TN} = $ \"True Negatives\" $ = \\text{Num. people Correctly labeled as Negative}$\n",
        "- $\\text{FN} = $ \"False Negatives\" $ = \\text{Num. people Wrongly labeled as Negative}$\n",
        "\n",
        "Which lets us express the Accuacy as\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugcIGIK_XDxC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "target = full_df['PINCP']\n",
        "full_df.drop(columns='PINCP', inplace=True)\n",
        "\n",
        "#note: here we are setting a value for the random_state (seed) parameter so that the results of this lab will remain consistent\n",
        "X_train, X_test, y_train, y_test = train_test_split(full_df, target, test_size=0.2, random_state=4)\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TODO 4**: What would be the accuracy ($\\frac{(TP + TN)}{(TP + FP + TN + FN)}$) of a classifier that always predicts the majority label (baseline classifier)?\n",
        "Since the majority label is 0, the accuracy of this majority label classifier would be the number of 0's over the total number of records.\n"
      ],
      "metadata": {
        "id": "G4otIko99As5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your work here:"
      ],
      "metadata": {
        "id": "ZNi0kX4CBOyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NlbkMZZEoFxG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "# Give the students the following:\n",
        "# y_test.value_counts()\n",
        "# # print(f'General baseline accuracy: {baseline_accuracy:.4f}')\n",
        "\n",
        "target_zero = y_test.value_counts()[0]\n",
        "target_one = y_test.value_counts()[1]\n",
        "baseline_accuracy = target_zero / (target_zero+target_one)\n",
        "\n",
        "print(f'General baseline accuracy: {baseline_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TODO 5**: What would be the accuracy ($\\frac{(TP + TN)}{(TP + FP + TN + FN)}$) for the Male and Female groups considering a classifier that always predicts the majority label (baseline classifier) for each of these groups?\n"
      ],
      "metadata": {
        "id": "yA9jjuc-BXDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your work here:"
      ],
      "metadata": {
        "id": "qOhMjYUWBcOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FHt5VXJefZ8L",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "# # print(f'Male baseline accuracy: {male_baseline_accuracy:.4f}')\n",
        "# # print(f'Female baseline accuracy: {female_baseline_accuracy:.4f}')\n",
        "\n",
        "male_zero = y_test[X_test['SEX']==1].value_counts()[0]\n",
        "male_one = y_test[X_test['SEX']==1].value_counts()[1]\n",
        "male_baseline_accuracy = male_zero / (male_zero + male_one)\n",
        "print(f'Male baseline accuracy: {male_baseline_accuracy:.4f}')\n",
        "\n",
        "female_zero = y_test[X_test['SEX']==0].value_counts()[0]\n",
        "female_one = y_test[X_test['SEX']==0].value_counts()[1]\n",
        "female_baseline_accuracy = female_zero / (female_zero + female_one)\n",
        "print(f'Female baseline accuracy: {female_baseline_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6f1ZdiyXSE3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Implement logistic regression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "clf_accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f'Logistic Regression test accuracy: {clf_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9vugXFztlG8"
      },
      "source": [
        "## Evaluate fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlGbK5w4x7q-"
      },
      "source": [
        "Next, we will evaluate the fairness of our classifier on the test set. We first define the **selection rate** of the classifier on a group:\n",
        "$$\n",
        "\\text{Selection Rate} = \\frac{\\text{Number of People Classified Positive}}{\\text{Total Number of People}}\n",
        "$$\n",
        "We will compute the selection rate amongst men and amongst women, and compare them. The difference of their selection rates is called the **Demographic parity difference**, and the ratio of their rates is called the **Demographic parity ratio**.\n",
        "\n",
        "In general, if we have more than 2 classes,\n",
        "- The demographic parity difference is the difference of the largest and smallest selection rates, so it is always positive. A demographic parity difference of 0 means that all groups have the same selection rate.\n",
        "\n",
        "- The demographic parity ratio is the ratio of the smallest to largest selection rates, so it is always between 0 and 1, where a ratio of 1 means that all groups have the same selection rate.\n",
        "\n",
        "<!-- Next, we will evaluate the fairness of our classifier on the test set.  We will first focus on two metrics - demographic parity difference and demographic parity ratio.  **Demographic parity difference** is defined as the difference between the largest and the smallest group-level selection rate across all values of the sensitive feature(s).  A demographic parity difference of 0 means that all groups have the same selection rate.  **Demographic parity ratio** is defined as the ratio between the smallest and the largest group-level selection rate across all values of the sensitive feature(s).  A demographic parity ratio of 1 means that all groups have the same selection rate. -->\n",
        "\n",
        "More formally: let $X$ denote a feature vector used for predictions, $A$ be a single sensitive feature (such as age or race), $Y$ be the true label, and $h$ a classifier or predictor resulted from a Machine Learning algorithm. Then:\n",
        "\n",
        "*Demographic Parity Difference* is defined as $(max_a\\mathbb{E}[h(X)~|~  A = a])~ - ~ (min_a\\mathbb{E}[h(X)~|~  A = a]) $\n",
        "\n",
        "\n",
        "*Demographic Parity Ratio* is defined as $\\frac{max_a\\mathbb{E}[h(X)~|~  A = a]}{min_a\\mathbb{E}[h(X)~|~  A = a]} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9-CPsT9bhhy",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Evaluate the fairness of the classifier using `demographic_parity_difference` and `demographic_parity_ratio`\n",
        "#note: we are conducting this analysis over the test set\n",
        "\n",
        "#compute test predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#compute demographic parity difference and demographic parity ratio\n",
        "demo_parity_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=X_test['SEX'])\n",
        "demo_parity_ratio = demographic_parity_ratio(y_test, y_pred, sensitive_features=X_test['SEX'])\n",
        "\n",
        "print(f'Demographic parity difference: {demo_parity_diff:.4f}')\n",
        "print(f'Demographic parity ratio: {demo_parity_ratio:.4f}')\n",
        "\n",
        "#compute selection rate for males and females\n",
        "male_selection_rate = selection_rate(y_test[X_test['SEX']==1], y_pred[X_test['SEX']==1])\n",
        "female_selection_rate = selection_rate(y_test[X_test['SEX']==0], y_pred[X_test['SEX']==0])\n",
        "\n",
        "print(f'Male selection rate: {male_selection_rate:.4f}')\n",
        "print(f'Female selection rate: {female_selection_rate:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbwDQ2nQ8c_J"
      },
      "source": [
        "Here, we can see that there are substantial differences in selection rates between males and females with males being significantly more likely to be classified as high income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxWyzIlKyHIH"
      },
      "source": [
        "Fairlearn also provides the fairlearn.metrics.MetricFrame class to evaluate disparities in treatment between different sub-populations.\n",
        "\n",
        "The **fairlearn.metrics.MetricFrame** object requires a minimum of four arguments:\n",
        "\n",
        "*   The underlying metric function(s) to be evaluated\n",
        "*   The true values\n",
        "*   The predicted values\n",
        "*   The sensitive feature values\n",
        "\n",
        "Metric functions must have a signature ''fn(y_true, y_pred)'', i.e., require only two arguments.  Here we will again look at selection rate, but we will also examine a few other metrics.  We will use accuracy, selection rate, false negative rate, and false positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lhzxtUcmay1"
      },
      "outputs": [],
      "source": [
        "# Evaluate the fairness of the classifier using the MetricFrame class for the `SEX` variable\n",
        "\n",
        "#changing sensitive feature inputs to be 'male' and 'female' instead of 1 and 0\n",
        "sensitive_feature_sex = X_test['SEX'].replace({0:'female', 1:'male'})\n",
        "\n",
        "#evaluation metrics\n",
        "metrics = {'accuracy': skm.accuracy_score,\n",
        "           'selection_rate': selection_rate,  # i.e., the percentage of the population which have â1â as their predicted label\n",
        "           'FNR': false_negative_rate,\n",
        "           'FPR': false_positive_rate\n",
        "           }\n",
        "\n",
        "grouped_on_sex = MetricFrame(metrics=metrics,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_pred,\n",
        "                             sensitive_features=sensitive_feature_sex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-AAN3eYyOrh"
      },
      "source": [
        "The **fairlearn.metrics.MetricFrame** has the **overall** property, which evaluates the metrics on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPuojgxHm4EG"
      },
      "outputs": [],
      "source": [
        "grouped_on_sex.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO 6**: Evaluate fairness for the `RAC1P` variable (for black and white individuals at minimum), and show `grouped_on_race.overall`\n"
      ],
      "metadata": {
        "id": "y4BCINzEB0Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your work here:"
      ],
      "metadata": {
        "id": "x3EGFfrgB6bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG4i1QxPYDm9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "sensitive_feature_race = (X_test['RAC1P_1'] + 2 * X_test['RAC1P_2'] ).replace({0:'other', 1:'white', 2:'black'})\n",
        "indices = sensitive_feature_race != 'other'\n",
        "#evaluation metrics\n",
        "metrics = {'accuracy': skm.accuracy_score,\n",
        "           'selection_rate': selection_rate,  # i.e., the percentage of the population which have â1â as their predicted label\n",
        "           'FNR': false_negative_rate,\n",
        "           'FPR': false_positive_rate\n",
        "           }\n",
        "\n",
        "grouped_on_race = MetricFrame(metrics=metrics,\n",
        "                             y_true=y_test[indices],\n",
        "                             y_pred=y_pred[indices],\n",
        "                             sensitive_features=sensitive_feature_race[indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mITEvRncj68m"
      },
      "outputs": [],
      "source": [
        "print(y_pred[indices].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD4OsxeWbXjW"
      },
      "outputs": [],
      "source": [
        "grouped_on_race.overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-EFAs8yQBd"
      },
      "source": [
        "The **fairlearn.metrics.MetricFrame** object also has the **by_group** functionality. This displays the selected metrics evaluated on each subgroup defined by the categories in the sensitive_features (sex in our case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4KM3xmxm9Ng",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "grouped_on_sex.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO 7**: Display for `RAC1P` as well"
      ],
      "metadata": {
        "id": "Nft2D0xUCDwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your work here:"
      ],
      "metadata": {
        "id": "TGMGBT9cCIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LaYBo98cbbJK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Answer\n",
        "grouped_on_race.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNr0r1yIIE3Y"
      },
      "source": [
        "Reminder: Females are labeled 0 and males are labeled 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9z7HUdm_up",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Plot the metric values\n",
        "\n",
        "metrics_1 = {'accuracy': skm.accuracy_score,\n",
        "           'selection_rate': selection_rate,  # i.e., the percentage of the population which have â1â as their predicted label\n",
        "           }\n",
        "\n",
        "metrics_2 = {\n",
        "           'FNR': false_negative_rate,\n",
        "           'FPR': false_positive_rate\n",
        "           }\n",
        "\n",
        "grouped_on_sex_accuracy_selection = MetricFrame(metrics=metrics_1,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_pred,\n",
        "                             sensitive_features=sensitive_feature_sex)\n",
        "\n",
        "grouped_on_sex_fpr_fnr = MetricFrame(metrics=metrics_2,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_pred,\n",
        "                             sensitive_features=sensitive_feature_sex)\n",
        "\n",
        "grouped_on_sex_accuracy_selection.by_group.plot.bar(\n",
        "    subplots=False,\n",
        "    figsize=(10, 7),\n",
        "    ylim=[0,1],\n",
        "    title=\"Accuracy and selection rate by sex\",\n",
        "    )\n",
        "\n",
        "grouped_on_sex_fpr_fnr.by_group.plot.bar(\n",
        "    subplots=False,\n",
        "    figsize=(10, 7),\n",
        "    ylim=[0,1],\n",
        "    title=\"FNR and FPR by sex\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TODO 8**: Replicate the above plots for at `RAC1P` and at least black/white individuals in the data."
      ],
      "metadata": {
        "id": "Le-jOMssCNF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here:"
      ],
      "metadata": {
        "id": "mYkR6AGkCSCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK3tfGI-bjAy",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "grouped_on_race_accuracy_selection = MetricFrame(metrics=metrics_1,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_pred,\n",
        "                             sensitive_features=sensitive_feature_race)\n",
        "\n",
        "grouped_on_race_fpr_fnr = MetricFrame(metrics=metrics_2,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_pred,\n",
        "                             sensitive_features=sensitive_feature_race)\n",
        "\n",
        "grouped_on_race_accuracy_selection.by_group.plot.bar(\n",
        "    subplots=False,\n",
        "    figsize=(10, 7),\n",
        "    ylim=[0,1],\n",
        "    title=\"Accuracy and selection rate by race\",\n",
        "    )\n",
        "\n",
        "grouped_on_race_fpr_fnr.by_group.plot.bar(\n",
        "    subplots=False,\n",
        "    figsize=(10, 7),\n",
        "    ylim=[0,1],\n",
        "    title=\"FNR and FPR by race\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH7SZrv2N8ot"
      },
      "source": [
        "# **TODO 9**: Observe that accuracy for male and female groups is comparable, and yet we see disparities in FPR and FNR.  Which group benefits from the discrepancies in FPR and FNR shown above?  If you were deploying this system how would you measure performance (eg. accuracy, FNR, FPR)? (Reminder: females are labelled 0, males are labelled 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "rrK8z_xKCaX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Answer\n",
        "'''\n",
        "<Males benefit from both a lower false negative rate and a higher false positive rate.  They are less likely to be incorrectly classified\n",
        "as \"low income\" and are also more likely to be incorrectly classified as \"high income\".  When designing a classifier we would want to look\n",
        "beyond accuracy and selection rate and consider FPR and FNR.'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T5Zq_gseCce2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFqCzXyXyhcO"
      },
      "source": [
        "#3. Train \"Blind\" Logistic Regression Classifier (Fairness through Blindness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJPVQ7byn53"
      },
      "source": [
        "Next, we will remove the protected attribute of \"sex\" from our data and see what effect this has on the performance of our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGZewKk9tBbZ"
      },
      "outputs": [],
      "source": [
        "#Remove sensitive attribute from data\n",
        "X_train_blind = X_train.drop(columns='SEX')\n",
        "X_test_blind = X_test.drop(columns='SEX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-0MCOFjtL9l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Implement logistic regression\n",
        "clf_blind = LogisticRegression()\n",
        "clf_blind.fit(X_train_blind, y_train)\n",
        "clf_blind_accuracy = clf_blind.score(X_test_blind, y_test)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f'Logistic Regression test accuracy (without sensitive attribute): {clf_blind_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Svx5AUycMC5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Recall the accuracy fairness of the original classifier\n",
        "print('Recall the original classifier:')\n",
        "print(f'Logistic Regression test accuracy: {clf_accuracy:.4f}')\n",
        "print(f'Demographic parity difference: {demo_parity_diff:.4f}')\n",
        "print(f'Demographic parity ratio: {demo_parity_ratio:.4f}')\n",
        "print(f'Male selection rate: {male_selection_rate:.4f}')\n",
        "print(f'Female selection rate: {female_selection_rate:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEg0YU9Ltbzh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Evaluate fairness of the blind classifier\n",
        "\n",
        "#compute test predictions\n",
        "y_pred_blind = clf_blind.predict(X_test_blind)\n",
        "\n",
        "#compute demographic parity difference and demographic parity ratio\n",
        "demo_parity_diff_blind = demographic_parity_difference(y_test, y_pred_blind, sensitive_features=X_test['SEX'])\n",
        "demo_parity_ratio_blind = demographic_parity_ratio(y_test, y_pred_blind, sensitive_features=X_test['SEX'])\n",
        "\n",
        "print(f'Demographic parity difference (without sensitive attribute): {demo_parity_diff_blind:.4f}')\n",
        "print(f'Demographic parity ratio (without sensitive attribute): {demo_parity_ratio_blind:.4f}')\n",
        "\n",
        "#compute selection rate for males and females\n",
        "male_selection_rate_blind = selection_rate(y_test[X_test['SEX']==1], y_pred_blind[X_test['SEX']==1])\n",
        "female_selection_rate_blind = selection_rate(y_test[X_test['SEX']==0], y_pred_blind[X_test['SEX']==0])\n",
        "\n",
        "print(f'Male selection rate (without sensitive attribute): {male_selection_rate_blind:.4f}')\n",
        "print(f'Female selection rate (without sensitive attribute): {female_selection_rate_blind:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yueobvhd78gp"
      },
      "source": [
        "# **TODO 10:** describe the differences of both models in terms of accuracy and fairness among male and female groups?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your work here:"
      ],
      "metadata": {
        "id": "Y7nVQqVuC3r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Answer\n",
        "'''Here, we can see that removing the protected attribute from our data had very little impact on the model's accuracy. On the other hand,\n",
        "the fairness metrics improved considerably as the Demographic Parity Difference decreased and the Demographic Parity Ratio increased.'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qzR6FjRVC48Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRVVozeK0IYV"
      },
      "source": [
        "**Nevertheless**, we see that removing the protected feature did not eliminate the biases within our classifier, as exhibited below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FP75qKmuC6_"
      },
      "outputs": [],
      "source": [
        "#Evaluate the biases of the classifier using the MetricFrame class\n",
        "grouped_on_sex_blind = MetricFrame(metrics=metrics,\n",
        "                                     y_true=y_test,\n",
        "                                     y_pred=y_pred_blind,\n",
        "                                     sensitive_features=sensitive_feature_sex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0l6FUGluMWh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "grouped_on_sex_blind.overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwAXmgKcuR0M",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "grouped_on_sex_blind.by_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zENn3IQzsNjR"
      },
      "source": [
        "## Showing all the metrics for the full data and blinded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TOyiVARo16T",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Comparing the results: full-data vs. blinded data\n",
        "\n",
        "#demographic parity difference\n",
        "demo_parity_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=X_test['SEX'])\n",
        "demo_parity_diff_blind = demographic_parity_difference(y_test, y_pred_blind, sensitive_features=X_test['SEX'])\n",
        "\n",
        "#demographic parity ratio\n",
        "demo_parity_ratio = demographic_parity_ratio(y_test, y_pred, sensitive_features=X_test['SEX'])\n",
        "demo_parity_ratio_blind = demographic_parity_ratio(y_test, y_pred_blind, sensitive_features=X_test['SEX'])\n",
        "\n",
        "#selection rate\n",
        "male_selection_rate = selection_rate(y_test[X_test['SEX']==1], y_pred[X_test['SEX']==1])\n",
        "male_selection_rate_blind = selection_rate(y_test[X_test['SEX']==1], y_pred_blind[X_test['SEX']==1])\n",
        "\n",
        "female_selection_rate = selection_rate(y_test[X_test['SEX']==0], y_pred[X_test['SEX']==0])\n",
        "female_selection_rate_blind = selection_rate(y_test[X_test['SEX']==0], y_pred_blind[X_test['SEX']==0])\n",
        "\n",
        "#fnr\n",
        "male_fnr = false_negative_rate(y_test[X_test['SEX']==1], y_pred[X_test['SEX']==1])\n",
        "male_fnr_blind = false_negative_rate(y_test[X_test['SEX']==1], y_pred_blind[X_test['SEX']==1])\n",
        "\n",
        "female_fnr = false_negative_rate(y_test[X_test['SEX']==0], y_pred[X_test['SEX']==0])\n",
        "female_fnr_blind = false_negative_rate(y_test[X_test['SEX']==0], y_pred_blind[X_test['SEX']==0])\n",
        "\n",
        "#fpr\n",
        "male_fpr = false_positive_rate(y_test[X_test['SEX']==1], y_pred[X_test['SEX']==1])\n",
        "male_fpr_blind = false_positive_rate(y_test[X_test['SEX']==1], y_pred_blind[X_test['SEX']==1])\n",
        "\n",
        "female_fpr = false_positive_rate(y_test[X_test['SEX']==0], y_pred[X_test['SEX']==0])\n",
        "female_fpr_blind = false_positive_rate(y_test[X_test['SEX']==0], y_pred_blind[X_test['SEX']==0])\n",
        "\n",
        "#plot\n",
        "labels = ['Demo Parity Diff','Demo Parity Ratio','Sel Rate (Male)',\n",
        "          'Sel Rate (Female)', 'FNR (Male)', 'FNR (Female)', 'FPR(Male)',\n",
        "          'FPR(Female)']\n",
        "\n",
        "Y_full = [demo_parity_diff, demo_parity_ratio, male_selection_rate,\n",
        "          female_selection_rate, male_fnr, female_fnr, male_fpr, female_fpr]\n",
        "\n",
        "Y_blind = [demo_parity_diff_blind, demo_parity_ratio_blind,\n",
        "           male_selection_rate_blind, female_selection_rate_blind, male_fnr_blind,\n",
        "           female_fnr_blind, male_fpr_blind, female_fpr_blind]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 7))\n",
        "rects1 = ax.bar(x - width/2, Y_full, width, label='Full Model')\n",
        "rects2 = ax.bar(x + width/2, Y_blind, width, label='Blind Model')\n",
        "\n",
        "ax.set_title('Metrics Comparison', size=20)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend(fontsize='x-large')\n",
        "ax.bar_label(rects1, padding=3, fmt='%.3f')\n",
        "ax.bar_label(rects2, padding=3, fmt='%.3f')\n",
        "ax.set_ylim([0, 1])\n",
        "fig.tight_layout()\n",
        "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "Bias in AI Systems: The lab began by examining the COMPAS toolâs racial and gender biases in recidivism predictions, illustrating the need for fairness in AI.\n",
        "\n",
        "Reproducing Analysis: Participants replicated ProPublicaâs findings using the COMPAS dataset, focusing on logistic regression to detect bias patterns.\n",
        "\n",
        "Fairness with Fairlearn: The lab introduced Fairlearn for evaluating and mitigating biases in machine learning models, using the ACS dataset to address fairness in predictions related to sex.\n",
        "\n",
        "Mitigating Bias: Practical steps for improving model fairness were discussed, including fairness-aware modeling and adjustments to balance performance and equity.\n",
        "\n",
        "Responsible AI: The lab underscores the importance of addressing bias in AI systems and provides tools and insights for implementing responsible AI practices\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JI0slRqXFyXZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}