{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Building recommender systems using GNNs**\n",
    "\n",
    "This tutorial assumes that you have finished the graph neural networks and recommender systems tutorials. In this section of the tutorial we will demonstrate how we can use graph neural networks in recommender systems.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- Familiarity with Jax, especially flax & jraph\n",
    "- A grasp on the basics of neural networks\n",
    "- To have finished the graph neural networks (GNNs) tutorial\n",
    "- To have finished the recommender systems tutorial\n",
    "\n",
    "**Aims/Learning Objectives:**\n",
    "- Frame link prediction tasks within the context of movie recommendations\n",
    "- Implementation of GCN for movie recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Introduction**\n",
    "\n",
    "Graphs are a powerful and general representation of data with a wide range of applications. Most people are familiar with their use in contexts like social networks and biological systems. Another use case for graphs is in the realm of recommender systems which is it the focus of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. Graph Prediction Tasks**\n",
    "\n",
    "As demonstrated in the GNN tutorial there are three graph tasks in the context of graph neural networks:\n",
    "\n",
    "1. **Node Classification**: E.g. what is the topic of a paper given a citation network of papers?\n",
    "2. **Link Prediction / Edge Classification**: E.g. are two people in a social network friends?\n",
    "3. **Graph Classification**: E.g. is this protein molecule (represented as a graph) likely going to be effective?\n",
    "\n",
    "<image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_tasks.png\" width=\"700px\">\n",
    "\n",
    "*Image source: Petar Veličković.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Recommender systems as a link prediction problem**\n",
    "\n",
    " A recommender system can be visualized as a graph, where entities (such as users and items) are nodes, and the interactions between them (such as ratings or purchase history) are edges. In the context of a movie recommendation system:\n",
    "\n",
    "- Nodes might represent:\n",
    "    - Users: Individuals consuming the content.\n",
    "    - Movies: Content items to be recommended.\n",
    "- Edges represent:\n",
    "    - Ratings: A directed edge from a user to a movie, annotated with a weight that indicates the rating (e.g., on a scale of 1 to 5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1. Link (or Edge) Prediction**\n",
    "\n",
    "Link prediction tries to predict whether a link (or edge) should exist between two nodes, even if it's currently absent. For our movie recommendation system, this translates to predicting whether a user would like (or dislike) a movie they haven't yet rated.\n",
    "\n",
    "The process works as follows:\n",
    "1. Train on existing edges: Use known ratings (edges) from users to movies to train a model.\n",
    "2. Predict missing edges: For a given user, predict ratings for movies they haven't seen or rated. This is akin to predicting missing or potential edges in our graph.\n",
    "3. Recommend based on predictions: Movies with the highest predicted ratings are recommended to the user.\n",
    "\n",
    "In this section of the tutorial, we will leverage graph neural networks (GNNs) for this link prediction task. The GNNs operate on the graph, aggregating information from neighboring nodes to produce accurate predictions for unseen edges. The below figure showcases a user-item sub-knowledge graph. Each icon denotes an entity or concept, and the connecting lines or edges symbolize the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Installations\n",
    "%%capture\n",
    "!pip install annoy clu\n",
    "!pip install git+https://github.com/deepmind/jraph.git\n",
    "!pip install torch-geometric\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and import anything required. Capture hides the output from the cell.\n",
    "# @title Import required packages. (Run Cell)\n",
    "\n",
    "import gc\n",
    "import jax\n",
    "import jraph\n",
    "import optax\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from flax import struct\n",
    "from clu import metrics\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "import jax.tree_util as tree\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from jraph import GraphConvolution\n",
    "from flax.training import train_state\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from typing import Iterable, Mapping, Sequence, Tuple, Callable\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Description](graph_rc.png)\n",
    "\n",
    "*Image source: [HI2Rec: Exploring Knowledge in Heterogeneous Information for Movie Recommendation](https://www.semanticscholar.org/paper/HI2Rec%3A-Exploring-Knowledge-in-Heterogeneous-for-He-Wang/038eb4e6839352c8fa8f9c4f5ae5ff958e14c5a3)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 Data download**\n",
    "\n",
    "We have already preprocessed the data as a graph. If you are interested to have a better understanding of how we converted the initial movies rating data into a graph, feel free to read this tutorial section 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Explore data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use graph convolution neural network (GCN) as our model. For more in-depth understanding of GCN, please go throught the above provided resources in your free time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model\n",
    "class MLP(nn.Module):\n",
    "  \"\"\"A flax MLP.\"\"\"\n",
    "  features: Sequence[int]\n",
    "  kernel_init: Callable = jax.nn.initializers.he_uniform()\n",
    "  bias_init: Callable = jax.nn.initializers.zeros\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, feat in enumerate(self.features):\n",
    "        lyr = nn.Dense(feat, kernel_init=self.kernel_init, bias_init=self.bias_init, name=f\"mlp_dense_{i}\")\n",
    "        x = lyr(x)\n",
    "        x = nn.relu(x)\n",
    "        if i != len(self.features) - 1:\n",
    "            x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def make_embed_fn(latent_size):\n",
    "    def embed(inputs):\n",
    "        return nn.Dense(latent_size)(inputs)\n",
    "    return embed\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "\n",
    "  output_decoder_dim: int\n",
    "  latent_size: int\n",
    "  update_node_fn: Callable\n",
    "  aggregate_nodes_fn: Callable = jax.ops.segment_sum\n",
    "  add_self_edges: bool = False\n",
    "  symmetric_normalization: bool = True\n",
    "  layer_norm: bool = False\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, graph):\n",
    "    gcn = GraphConvolution(\n",
    "        update_node_fn=self.update_node_fn,\n",
    "        aggregate_nodes_fn=self.aggregate_nodes_fn,\n",
    "        add_self_edges=self.add_self_edges,\n",
    "        symmetric_normalization=self.symmetric_normalization\n",
    "    )\n",
    "    graph = gcn(graph)\n",
    "    if self.layer_norm:\n",
    "      # Apply layer normalization to the node embeddings\n",
    "      normalized_nodes = nn.LayerNorm()(graph.nodes)\n",
    "      # Update the graph with the normalized node embeddings\n",
    "      graph = graph._replace(nodes=normalized_nodes)\n",
    "\n",
    "    edge_predictions = jnp.sum(graph.nodes[graph.senders] * graph.nodes[graph.receivers], axis=-1)\n",
    "    edge_predictions = jnp.expand_dims(edge_predictions, axis=1)\n",
    "    edge_predictions = nn.Dense(self.output_decoder_dim, name=\"mlp_dense_output\")(edge_predictions)\n",
    "\n",
    "    # Apply sigmoid activation and scale it\n",
    "    edge_predictions = 4 * jax.nn.sigmoid(edge_predictions) + 1\n",
    "    return edge_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one layer gcn\n",
    "gcn_layer = GraphConvLayer(\n",
    "    output_decoder_dim = 1,\n",
    "    latent_size = 64,\n",
    "    update_node_fn=lambda n: MLP(features=[64, 64])(n),\n",
    "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
    "    add_self_edges=False,\n",
    "    symmetric_normalization=True\n",
    ")\n",
    "\n",
    "# Initialize to see the output shapes\n",
    "params = gcn_layer.init(jax.random.PRNGKey(42), graph)\n",
    "output = gcn_layer.apply(params, graph)\n",
    "output.shape, graph.edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def l1_loss(logits: np.ndarray, y: np.ndarray, mask: np.array, reduction: str = \"mean\") -> np.ndarray:\n",
    "    \"\"\"Implementation of l1_loss.\n",
    "\n",
    "    Args:\n",
    "        logits: model output logits.\n",
    "        y: class labels.\n",
    "        reduction: if reduction is mean, the average is returned, else if it is sum, the sum is returned.\n",
    "\n",
    "    Returns:\n",
    "       l1 loss.\n",
    "    \"\"\"\n",
    "    logits_masked = logits * mask.astype(int)\n",
    "    preds_masked = y * mask.astype(int)\n",
    "    if reduction == \"mean\":\n",
    "        loss = jnp.mean(jnp.abs(logits_masked - preds_masked))\n",
    "    if reduction == \"sum\":\n",
    "        loss = jnp.sum(jnp.abs(logits_masked - preds_masked))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# check random loss without training\n",
    "l1_loss(output, graph.edges, mask=val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define flax train state\n",
    "def create_train_state(\n",
    "    model, graph, tx, rngs\n",
    "):\n",
    "    \"\"\"Train state. This function initializes the model.\"\"\"\n",
    "\n",
    "    @jax.jit\n",
    "    def initialize(params_rng):\n",
    "        variables = model.init(\n",
    "            params_rng,\n",
    "            graph,\n",
    "        )\n",
    "        return variables\n",
    "\n",
    "    variables = initialize(rngs)\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=variables[\"params\"], tx=tx)\n",
    "\n",
    "    param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "    print(\"---> number of model parameters: \", param_count)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, train state, and other hparams\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "rngs = {\"params\": rng, \"dropout\": init_rng}\n",
    "\n",
    "model = GraphConvLayer(\n",
    "    output_decoder_dim = 1,\n",
    "    latent_size = 128,\n",
    "    update_node_fn=lambda n: MLP(features=[128, 128, 64])(n),\n",
    "    aggregate_nodes_fn=jax.ops.segment_sum,\n",
    "    add_self_edges=True,\n",
    "    layer_norm=True,\n",
    "    symmetric_normalization=True\n",
    ")\n",
    "state = create_train_state(\n",
    "    model=model,\n",
    "    graph=graph,\n",
    "    tx=optimizer,\n",
    "    rngs=rngs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our model has 27458 parameters. You can play with this by changing hyper-parameters such as latent_size, MLP features, etc. Next, we will define train and evaluation steps and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train step\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState,\n",
    "    graph: jnp.array,\n",
    "    labels: jnp.ndarray,\n",
    "    mask: jnp.ndarray,\n",
    "    rngs: dict,\n",
    ") -> Tuple[train_state.TrainState, tuple]:\n",
    "    \"\"\"Performs one update step over the graph.\n",
    "\n",
    "    Args:\n",
    "        state: training state.\n",
    "        graph: graph node features.\n",
    "        labels: graph edge labels.\n",
    "        mask: mask for labels to consider for optimization\n",
    "        rngs: rngs for droupout\n",
    "\n",
    "    Returns:\n",
    "        Current training state, the loss, and logits.\n",
    "    \"\"\"\n",
    "    step = state.step\n",
    "    rngs = {name: jax.random.fold_in(rng, step) for name, rng in rngs.items()}\n",
    "\n",
    "    def loss_fn(params, graph, labels):\n",
    "        # Compute logits and resulting loss.\n",
    "        variables = {\"params\": params}\n",
    "        logits = state.apply_fn(\n",
    "            variables,\n",
    "            graph=graph,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        loss = l1_loss(logits=logits, y=labels, mask=mask)\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params, graph, labels)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, (loss, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Evaluation step\n",
    "@jax.jit\n",
    "def evaluate_step(\n",
    "    state: train_state.TrainState,\n",
    "    graph: jnp.array,\n",
    "    labels: jnp.ndarray,\n",
    "    mask: jnp.ndarray,\n",
    "    dropout_rng: dict = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Performs evaluation step over a set of inputs.\"\"\"\n",
    "    variables = {\"params\": state.params}\n",
    "    logits = state.apply_fn(\n",
    "        variables,\n",
    "        graph=graph,\n",
    "        rngs=dropout_rng,\n",
    "    )\n",
    "    loss = l1_loss(logits=logits, y=labels, mask=mask)\n",
    "    return (loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train loop function\n",
    "def train_eval(state, graph, train_mask, val_mask, rng, epochs = 10):\n",
    "    final_train_loss = []\n",
    "    final_val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        rng, epoch_rng = jax.random.split(rng)\n",
    "        epoch_rng = {\"dropout\": epoch_rng}\n",
    "\n",
    "        state, (train_loss, train_logits) = train_step(state=state, graph=graph, labels=graph.edges, mask=train_mask, rngs = epoch_rng)\n",
    "        val_loss, val_logits = evaluate_step(state=state, graph=graph, mask=val_mask, labels=graph.edges)\n",
    "        print(f\"Epoch: {epoch}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "        final_train_loss.append(train_loss.item())\n",
    "        final_val_loss.append(val_loss.item())\n",
    "\n",
    "    return (state, final_train_loss, final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 100\n",
    "final_state, final_train_loss, final_val_loss = train_eval(state=state, graph=graph, train_mask=train_mask, val_mask=val_mask, rng=rng, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), final_train_loss, label=\"Train Loss\", marker='o')\n",
    "plt.plot(range(epochs), final_val_loss, label=\"Validation Loss\", marker='*')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Inference**\n",
    "\n",
    "In this section, we will show how we can recommend top N movies for a specific user based on the trained GNN. Since we already have a test set edges. We will use this to demonstrate the inference. We will do the following:\n",
    "\n",
    "1. Extract test edges: With the test_mask, obtain the edges that belong to the test set.\n",
    "2. Predict ratings: For each of these test edges, use the trained GNN to predict a rating.\n",
    "3. Map to original data: Using the movie_mapping and user_mapping from the GraphDataPreparation class, convert the predicted edges back to the original movie and user IDs.\n",
    "4. Get movie names: For the mapped movie IDs, fetch the actual movie names.\n",
    "5. To get the top N recommended movies for a particular user, we will sort the movies based on the predicted ratings and then pick the top N movies for the user.\n",
    "\n",
    "\n",
    "Below we will implement two functions to do the above steps for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_movie_ratings(graph, state, test_mask, data_prep: GraphDataPreparation):\n",
    "    \"\"\"\n",
    "    Infer movie ratings for the test set and map them to the original movies and users.\n",
    "\n",
    "    Args:\n",
    "        graph: The graph containing nodes and edges.\n",
    "        state: The trained state of the model.\n",
    "        test_mask: The mask indicating which edges belong to the test set.\n",
    "        data_prep: The GraphDataPreparation instance used to process the data.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing user IDs, movie names, and predicted ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Using the test_mask, get the edges corresponding to the test set\n",
    "    test_edge_senders = graph.senders[test_mask.squeeze()]\n",
    "    test_edge_receivers = graph.receivers[test_mask.squeeze()] - len(data_prep.user_mapping)\n",
    "\n",
    "    # Predict the ratings using the trained model\n",
    "    _, logits = evaluate_step(state, graph, labels=graph.edges, mask=test_mask)\n",
    "    predicted_ratings = logits[test_mask]\n",
    "\n",
    "    # Map back to original user and movie IDs\n",
    "    reverse_user_mapping = {v: k for k, v in data_prep.user_mapping.items()}\n",
    "    reverse_movie_mapping = {v: k for k, v in data_prep.movie_mapping.items()}\n",
    "\n",
    "    original_user_ids = np.array([reverse_user_mapping[v] for v in test_edge_senders]).astype(int)\n",
    "    original_movie_ids = np.array([reverse_movie_mapping[v] for v in test_edge_receivers]).astype(int)\n",
    "\n",
    "    # Revert the remapping on the movie_id column of the movies_df\n",
    "    movies_df_original = data_prep.movies_df.copy()\n",
    "    movies_df_original['movie_id'] = movies_df_original['movie_id'].map(reverse_movie_mapping).astype(int)\n",
    "\n",
    "    # Get movie names using the original movie_ids\n",
    "    movie_title = movies_df_original.set_index('movie_id').loc[original_movie_ids, 'movie_title'].values\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'user_id': original_user_ids,\n",
    "        'movie_name': movie_title,\n",
    "        'predicted_rating': np.round(predicted_ratings, 3)\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "predicted_ratings_df = infer_movie_ratings(graph, final_state, test_mask, graph_prep)\n",
    "predicted_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(df, user_id, N=10):\n",
    "    \"\"\"\n",
    "    Get the top N recommended movies for a user.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing user IDs, movie title, and predicted ratings.\n",
    "        user_id (int): The user ID for whom the recommendations are to be fetched.\n",
    "        N (int): The number of top movies to fetch.\n",
    "\n",
    "    Returns:\n",
    "        A list of top N movie title for the user.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter out the movies for the given user and sort them based on predicted ratings in descending order\n",
    "    top_movies = df[df['user_id'] == user_id].sort_values(by='predicted_rating', ascending=False).head(N)\n",
    "\n",
    "    return top_movies['movie_name'].tolist()\n",
    "\n",
    "# choose user ID to get it's N ratings\n",
    "user_id = 600\n",
    "N = 10\n",
    "top_10_movies = get_top_n_recommendations(predicted_ratings_df, user_id, N)\n",
    "print(f'Top {len(top_10_movies)} movies recommended for user ID: {user_id} are: ')\n",
    "\n",
    "for number, movie_title in enumerate(top_10_movies):\n",
    "    print(f'{number}: {movie_title}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indaba_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
